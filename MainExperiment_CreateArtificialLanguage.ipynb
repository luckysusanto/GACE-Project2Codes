{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data For Artificial Language Experiments"
      ],
      "metadata": {
        "id": "zMUC0sfUa6zN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Tokenize and Recombine all used files before using them"
      ],
      "metadata": {
        "id": "tB3ZBqmTa_R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def recombine(list_of_str):\n",
        "  import string\n",
        "  ret = \"\"\n",
        "  prev = \"\"\n",
        "  for token in list_of_str:\n",
        "    if prev == \"-\":\n",
        "      ret = ret[:-1] + token + ' '\n",
        "    elif token in string.punctuation:\n",
        "      ret = ret[:-1] + token + ' '\n",
        "    else:\n",
        "      ret += token + ' '\n",
        "    prev = token\n",
        "  return ret[:-1]\n",
        "\n",
        "def normalize_file(f_name):\n",
        "  import re\n",
        "  import string\n",
        "\n",
        "  normalized = \"\"\n",
        "  with open(f_name, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      normalized += recombine(tokenize(line)) + '\\n'\n",
        "\n",
        "  with open(f_name, 'w') as f:\n",
        "    f.write(normalized)\n",
        "  return normalized"
      ],
      "metadata": {
        "id": "flLeKmudbW1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "for corpus in corpora:\n",
        "  normalize_file(corpus)"
      ],
      "metadata": {
        "id": "UJq4dVoJbJNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Noising\n",
        "Use:\n",
        "1. A_noising --> function\n",
        "2. A_noising_map --> pickle"
      ],
      "metadata": {
        "id": "BSOEFs4VctrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def A_noising(corpus, noising_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = tokenize(sentence)\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = noising_map.get(tokens[i], tokens[i])\n",
        "\n",
        "    corrupted_sentence = recombine(tokens)\n",
        "    res += corrupted_sentence +'\\n'\n",
        "  return res\n",
        "\n",
        "import pickle\n",
        "noising_map = {}\n",
        "with open('A_noising_map.pickle', 'rb') as f:\n",
        "  noising_map = pickle.load(f)"
      ],
      "metadata": {
        "id": "fb1i8nk2c78D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_corpora = {}\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(corpus, 'r') as f:\n",
        "    A_corpora[f'A_{corpus}'] = A_noising(f.readlines(), noising_map)\n",
        "\n",
        "for filename in A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(A_corpora[filename])"
      ],
      "metadata": {
        "id": "EnLMoe1IdAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Cognates\n",
        "Use:\n",
        "1. B_compound_map --> pickle\n",
        "2. B_compounding --> function"
      ],
      "metadata": {
        "id": "cC_8KYYidxU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "compound_map = {}\n",
        "with open('B_compound_map.pickle', 'rb') as f:\n",
        "  compound_map = pickle.load(f)\n",
        "\n",
        "def B_compounding(corpus, compound_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = sentence.split(' ')\n",
        "    if len(tokens) == 1:\n",
        "      return str(tokens[0])\n",
        "\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "\n",
        "    for bigram_idx in range(len(bigrams)):\n",
        "      if bigrams[bigram_idx] in compound_map:\n",
        "        bigrams[bigram_idx] = (compound_map[bigrams[bigram_idx]], '')\n",
        "\n",
        "        if bigram_idx == 0:\n",
        "          bigrams[1] = ('', bigrams[1][1])\n",
        "        elif bigram_idx == len(bigrams) - 1:\n",
        "          bigrams[len(bigrams) - 2] = (bigrams[len(bigrams) - 2][0], '')\n",
        "        else:\n",
        "          bigrams[bigram_idx-1] = (bigrams[bigram_idx-1][0], '')\n",
        "          bigrams[bigram_idx+1] = ('', bigrams[bigram_idx+1][1])\n",
        "\n",
        "    reconstruct = []\n",
        "    for i in range(len(bigrams)):\n",
        "      if i != len(bigrams) - 1:\n",
        "        if bigrams[i][0] != '':\n",
        "          reconstruct.append(bigrams[i][0])\n",
        "      else:\n",
        "        if bigrams[i][0] != '':\n",
        "          reconstruct.append(bigrams[i][0])\n",
        "          reconstruct.append(bigrams[i][1])\n",
        "        else:\n",
        "          reconstruct.append(bigrams[i][1])\n",
        "    reconstruct = ' '.join(reconstruct)\n",
        "    res += reconstruct\n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "yRxLgrW8eJz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_corpora = {}\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(corpus, 'r') as f:\n",
        "    B_corpora[f'B_{corpus}'] = B_compounding(f.readlines(), compound_map)\n",
        "\n",
        "for filename in B_corpora.keys():\n",
        "  with open(f'{filename}', 'w') as f:\n",
        "    f.write(B_corpora[filename])"
      ],
      "metadata": {
        "id": "rXH9U4rretHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AB. A + B\n",
        "Use:\n",
        "1. B files --> file\n",
        "2. AB_noising_map --> pickle\n",
        "3. A_noising --> function\n"
      ],
      "metadata": {
        "id": "lcZ2STyRfVfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "AB_noising_map = {}\n",
        "with open('AB_noising_map.pickle', 'rb') as f:\n",
        "  AB_noising_map = pickle.load(f)"
      ],
      "metadata": {
        "id": "c432MtdmflGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AB_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'B_{corpus}', 'r') as f:\n",
        "    AB_corpora[f'AB_{corpus}'] = A_noising(f.readlines(), AB_noising_map)\n",
        "\n",
        "for filename in AB_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(AB_corpora[filename])"
      ],
      "metadata": {
        "id": "Vynt3URef0Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C/Dx + A\n",
        "Use:\n",
        "1. C1/C2/D1/D2_mapping.pickle --> 4 pickle files\n",
        "2. translate_corpora --> function\n",
        "3. ---> Results in 8 files for each mapping\n",
        "4. A_noising --> function\n",
        "5. C1/C2/D1/D2_noising_map.pickle --> 4 pickle files"
      ],
      "metadata": {
        "id": "vrNNijgMgfli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate corpus"
      ],
      "metadata": {
        "id": "ZSaIkoZKhvhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "import pickle\n",
        "with open(f'C1_mapping.pickle', 'rb') as f:\n",
        "  C1_map = pickle.load(f)\n",
        "\n",
        "with open(f'C2_mapping.pickle', 'rb') as f:\n",
        "  C2_map = pickle.load(f)\n",
        "\n",
        "with open(f'D1_mapping.pickle', 'rb') as f:\n",
        "  D1_map = pickle.load(f)\n",
        "\n",
        "with open(f'D2_mapping.pickle', 'rb') as f:\n",
        "  D2_map = pickle.load(f)\n",
        "\n",
        "def translate_corpora(code, files, mapping):\n",
        "  for file_ in files:\n",
        "    res = ''\n",
        "    with open(file_, 'r') as f:\n",
        "      for sentence in f:\n",
        "        tokens = tokenize(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "          tokens[i] = mapping.get(tokens[i], tokens[i])\n",
        "\n",
        "        corrupted_sentence = recombine(tokens)\n",
        "        res += corrupted_sentence +'\\n'\n",
        "\n",
        "    with open(f\"{code}_{file_}\", 'w') as f:\n",
        "      f.write(res)\n",
        "\n",
        "translate_corpora('C1', corpora, C1_map)\n",
        "translate_corpora('C2', corpora, C2_map)\n",
        "translate_corpora('D1', corpora, D1_map)\n",
        "translate_corpora('D2', corpora, D2_map)"
      ],
      "metadata": {
        "id": "lV8mvId_hDYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C1\n",
        "Use:\n",
        "1. A_noising --> function\n",
        "2. C1A_noising_map --> pickle"
      ],
      "metadata": {
        "id": "6D8rRUZThRTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('C1A_noising_map.pickle', 'rb') as f:\n",
        "  C1A_noising_map = pickle.load(f)\n",
        "\n",
        "C1A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'C1_{corpus}', 'r') as f:\n",
        "    C1A_corpora[f'C1A_{corpus}'] = A_noising(f.readlines(), C1A_noising_map)\n",
        "\n",
        "for filename in C1A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(C1A_corpora[filename])"
      ],
      "metadata": {
        "id": "p6swcv5AhfGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C2\n",
        "1. A_noising --> function\n",
        "2. C2A_noising_map --> pickle"
      ],
      "metadata": {
        "id": "lxgVC7xdhSua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('C2A_noising_map.pickle', 'rb') as f:\n",
        "  C2A_noising_map = pickle.load(f)\n",
        "\n",
        "C2A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'C2_{corpus}', 'r') as f:\n",
        "    C2A_corpora[f'C2A_{corpus}'] = A_noising(f.readlines(), C2A_noising_map)\n",
        "\n",
        "for filename in C2A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(C2A_corpora[filename])"
      ],
      "metadata": {
        "id": "TYyP80VNi9cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D1\n",
        "1. A_noising --> function\n",
        "2. D1A_noising_map --> pickle"
      ],
      "metadata": {
        "id": "lwkYofpGhT1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('D1A_noising_map.pickle', 'rb') as f:\n",
        "  D1A_noising_map = pickle.load(f)\n",
        "\n",
        "D1A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'D1_{corpus}', 'r') as f:\n",
        "    D1A_corpora[f'D1A_{corpus}'] = A_noising(f.readlines(), D1A_noising_map)\n",
        "\n",
        "for filename in D1A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(D1A_corpora[filename])"
      ],
      "metadata": {
        "id": "-GLEPPRujP_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D2\n",
        "1. A_noising --> function\n",
        "2. D2A_noising_map --> pickle"
      ],
      "metadata": {
        "id": "V9Zz3gX9hXRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('D2A_noising_map.pickle', 'rb') as f:\n",
        "  D2A_noising_map = pickle.load(f)\n",
        "\n",
        "D2A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'D2_{corpus}', 'r') as f:\n",
        "    D2A_corpora[f'D2A_{corpus}'] = A_noising(f.readlines(), D2A_noising_map)\n",
        "\n",
        "for filename in D2A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(D2A_corpora[filename])"
      ],
      "metadata": {
        "id": "QUPjgyBBjvLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C/Dx + B\n",
        "Uses:\n",
        "1. B_compounding --> function\n",
        "2. C/Dx_compound_map.pickle --> pickle"
      ],
      "metadata": {
        "id": "LKh7ke18kkk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C1"
      ],
      "metadata": {
        "id": "nRtNmD0ZlT8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "C1_compound_map = {}\n",
        "with open('C1B_compound_map.pickle', 'rb') as f:\n",
        "  C1_compound_map = pickle.load(f)\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'C1_{corpus}', 'r') as f, open(f'C1B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), C1_compound_map))"
      ],
      "metadata": {
        "id": "Fgum9JHglifL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C2"
      ],
      "metadata": {
        "id": "nqi-cuuWlVCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "C2_compound_map = {}\n",
        "with open('C2B_compound_map.pickle', 'rb') as f:\n",
        "  C2_compound_map = pickle.load(f)\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'C2_{corpus}', 'r') as f, open(f'C2B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), C2_compound_map))"
      ],
      "metadata": {
        "id": "OGVAvrDGmhCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D1"
      ],
      "metadata": {
        "id": "6-bU8smDlVtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "D1_compound_map = {}\n",
        "with open('D1B_compound_map.pickle', 'rb') as f:\n",
        "  D1_compound_map = pickle.load(f)\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'D1_{corpus}', 'r') as f, open(f'D1B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), D1_compound_map))"
      ],
      "metadata": {
        "id": "-Ya_7_UfmzwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D2"
      ],
      "metadata": {
        "id": "UCSqDjVIlWlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "D2_compound_map = {}\n",
        "with open('D2B_compound_map.pickle', 'rb') as f:\n",
        "  D2_compound_map = pickle.load(f)\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'D2_{corpus}', 'r') as f, open(f'D2B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), D2_compound_map))"
      ],
      "metadata": {
        "id": "-e4QecffnQUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C/Dx + B + A\n",
        "Uses:\n",
        "1. C/Dx + B files --> files\n",
        "2. A_noising --> function\n",
        "3. C/DxBA_noising_map --> pickle"
      ],
      "metadata": {
        "id": "TiKPoPiknjsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C1"
      ],
      "metadata": {
        "id": "v8unS6bMoMyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "C1BA_noising_map = {}\n",
        "with open('C1BA_noising_map.pickle', 'rb') as f:\n",
        "  C1BA_noising_map = pickle.load(f)\n",
        "\n",
        "C1BA_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'C1B_{corpus}', 'r') as f:\n",
        "    C1BA_corpora[f'C1BA_{corpus}'] = A_noising(f.readlines(), C1BA_noising_map)\n",
        "\n",
        "for filename in C1BA_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(C1BA_corpora[filename])"
      ],
      "metadata": {
        "id": "uq4EjDAToco7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C2"
      ],
      "metadata": {
        "id": "sTGu-KtQoN2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "C2BA_noising_map = {}\n",
        "with open('C2BA_noising_map.pickle', 'rb') as f:\n",
        "  C2BA_noising_map = pickle.load(f)\n",
        "\n",
        "C2BA_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'C2B_{corpus}', 'r') as f:\n",
        "    C2BA_corpora[f'C2BA_{corpus}'] = A_noising(f.readlines(), C2BA_noising_map)\n",
        "\n",
        "for filename in C2BA_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(C2BA_corpora[filename])"
      ],
      "metadata": {
        "id": "RDvq72w4pabq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D1"
      ],
      "metadata": {
        "id": "3aJnbO1hoPex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "D1BA_noising_map = {}\n",
        "with open('D1BA_noising_map.pickle', 'rb') as f:\n",
        "  D1BA_noising_map = pickle.load(f)\n",
        "\n",
        "D1BA_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'D1B_{corpus}', 'r') as f:\n",
        "    D1BA_corpora[f'D1BA_{corpus}'] = A_noising(f.readlines(), D1BA_noising_map)\n",
        "\n",
        "for filename in D1BA_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(D1BA_corpora[filename])"
      ],
      "metadata": {
        "id": "EouThWMtpvTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D2"
      ],
      "metadata": {
        "id": "hhfx778VoQxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "D2BA_noising_map = {}\n",
        "with open('D2BA_noising_map.pickle', 'rb') as f:\n",
        "  D2BA_noising_map = pickle.load(f)\n",
        "\n",
        "D2BA_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'D2B_{corpus}', 'r') as f:\n",
        "    D2BA_corpora[f'D2BA_{corpus}'] = A_noising(f.readlines(), D2BA_noising_map)\n",
        "\n",
        "for filename in D2BA_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(D2BA_corpora[filename])"
      ],
      "metadata": {
        "id": "uXISAdqsqHZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}