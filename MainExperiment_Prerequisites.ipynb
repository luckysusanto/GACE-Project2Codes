{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg1TJ6Exy-Pi"
      },
      "source": [
        "# Untokenize Word-Order-Shuffler results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki--xkRo5mKD"
      },
      "outputs": [],
      "source": [
        "original_f = 'flores_dev_english_svo'\n",
        "tokenized_f = 'flores_dev_english_vso'\n",
        "\n",
        "def untokenize_file(original_f, tokenized_f):\n",
        "  with open(f'prev_{tokenized_f}', 'w') as f, open(tokenized_f, 'r') as g:\n",
        "    f.write(g.read())\n",
        "\n",
        "  untokenized_text = \"\"\n",
        "  with open(original_f, 'r') as f, open(tokenized_f, 'r') as g:\n",
        "    zipped = zip(f.readlines(), g.readlines())\n",
        "    for original, tokenized_text in zipped:\n",
        "      untokenized_text += untokenize(original.strip(), tokenized_text.strip()) + ' \\n'\n",
        "\n",
        "  with open(tokenized_f, 'w') as f:\n",
        "    f.write(untokenized_text)\n",
        "\n",
        "untokenize_file(original_f, tokenized_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BvbKhuvKJ6_",
        "outputId": "d9d6ef7e-9f31-47f6-f800-b3543de16b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenized = ['During', 'the', '1976', 'selections', 'advised', 'then', 'Carter', 'he', 'on', 'foreign', 'policy', ',', 'served', 'as', 'National', 'Security', 'Advisor', '(', 'NSA', ')', 'from', '1977', 'to', '1981', ',', 'succeeding', 'Henry', 'Kissinger', '.']\n",
            "During the 1976 selections advised then Carter he on foreign policy , served as National Security Advisor ( NSA ) from 1977 to 1981 , succeeding Henry Kissinger .\n",
            "During the 1976 selections advised then Carter he on foreign policy, served as National Security Advisor( NSA) from 1977 to 1981, succeeding Henry Kissinger.\n"
          ]
        }
      ],
      "source": [
        "original = \"During the 1976 selections advised then Carter he on foreign policy , served as National Security Advisor ( NSA ) from 1977 to 1981 , succeeding Henry Kissinger .\"\n",
        "\n",
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def recombine(list_of_str):\n",
        "  import string\n",
        "  ret = \"\"\n",
        "  prev = \"\"\n",
        "  for token in list_of_str:\n",
        "    if prev == \"-\":\n",
        "      ret = ret[:-1] + token + ' '\n",
        "    elif token in string.punctuation:\n",
        "      ret = ret[:-1] + token + ' '\n",
        "    else:\n",
        "      ret += token + ' '\n",
        "    prev = token\n",
        "  return ret[:-1]\n",
        "\n",
        "def recombine_test(list_of_str):\n",
        "  def has_punctuation(input_string):\n",
        "      # Define a set of punctuation characters\n",
        "      punctuation = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "      # Iterate through the characters in the string\n",
        "      for char in input_string:\n",
        "          if char in punctuation:\n",
        "              return True\n",
        "      return False\n",
        "\n",
        "\n",
        "  import string\n",
        "  ret = \"\"\n",
        "  prev = \"\"\n",
        "  for token in list_of_str:\n",
        "    if prev == \"-\":\n",
        "      ret = ret[:-1] + token + ' '\n",
        "    elif has_punctuation(token):\n",
        "    # elif token in string.punctuation:\n",
        "      ret = ret[:-1] + token + ' '\n",
        "    else:\n",
        "      ret += token + ' '\n",
        "    prev = token\n",
        "  return ret[:-1]\n",
        "\n",
        "def untokenize(text):\n",
        "  return recombine_test(tokenize(text))\n",
        "\n",
        "print(f\"tokenized = {tokenize(original)}\")\n",
        "print(original)\n",
        "print(untokenize(original))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pFFDa6rzB1Z"
      },
      "source": [
        "# Word-level noise maker\n",
        "Noising:\n",
        "1. A. 5 percent of character level bigram\n",
        "2. B. Compounding\n",
        "\n",
        "Cognates:\n",
        "1. C1. German\n",
        "2. C2. Portuguese\n",
        "3. D1. Afrikaans\n",
        "4. D2. Galician\n",
        "\n",
        "List of Experiments:\n",
        "1. A\n",
        "2. B\n",
        "3. B + A\n",
        "4. C/Dx + A\n",
        "5. C/Dx + B\n",
        "6. C/Dx + B + A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNhkYwarD2Ov"
      },
      "source": [
        "## A. Noising\n",
        "Using 5% of character level bigram (bigram_5p.xlsx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDOAzR8l-Gyo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('bigram_5p.xlsx')\n",
        "translation_dict = df.set_index('Original')['Translation'].to_dict()\n",
        "\n",
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "A_corpora = {}\n",
        "\n",
        "def obtain_mapping(refs_file, translation_dict, default_noising_map = {}):\n",
        "  def tokenize(text):\n",
        "    import re\n",
        "    import string\n",
        "    ret = []\n",
        "    for token in text.split(' '):\n",
        "      result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "      flag = 0\n",
        "      for token in result_list:\n",
        "        if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "          flag = 1\n",
        "      if flag:\n",
        "        ret.append(''.join(result_list))\n",
        "      elif len(result_list) > 2:\n",
        "        ret.append(''.join(result_list))\n",
        "      else:\n",
        "        ret.extend(result_list)\n",
        "    return ret\n",
        "\n",
        "  noising_map = default_noising_map\n",
        "  for ref_file in refs_file:\n",
        "    with open(ref_file, 'r') as f:\n",
        "      for sentence in f.readlines():\n",
        "        tokens = tokenize(sentence)\n",
        "        split_2 = [[token[i:i+2] for i in range(0, len(token), 2)] for token in tokens]\n",
        "        for split_idx in range(len(split_2)):\n",
        "          split = split_2[split_idx]\n",
        "          for i in range(len(split)):\n",
        "            if split[i] in translation_dict:\n",
        "              split[i] = translation_dict[split[i]]\n",
        "\n",
        "        corrupted_tokens = [''.join(subtokens) for subtokens in split_2]\n",
        "        for ori, trans in zip(tokens, corrupted_tokens):\n",
        "          if noising_map.get(ori, 0) == 0:\n",
        "            noising_map[ori] = trans\n",
        "          elif noising_map[ori] != trans:\n",
        "            print(f\"noising_map[{ori}] = {noising_map[ori]}, not {trans}\")\n",
        "  return noising_map\n",
        "\n",
        "def A_noising(corpus, noising_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = tokenize(sentence)\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = noising_map[tokens[i]]\n",
        "\n",
        "    corrupted_sentence = recombine(tokens)\n",
        "    res += corrupted_sentence +'\\n'\n",
        "  return res\n",
        "\n",
        "# refs_file are one from devtest and one from dev, it doesn't matter which word-order is used\n",
        "refs_file = [\"flores_english_svo\", \"flores_dev_english_svo\"]\n",
        "noising_map = obtain_mapping(corpora, translation_dict)\n",
        "\n",
        "# for corpus in corpora:\n",
        "#   with open(corpus, 'r') as f:\n",
        "#     A_corpora[f'A_{corpus}'] = A_noising(f.readlines(), noising_map)\n",
        "\n",
        "# for filename in A_corpora.keys():\n",
        "#   with open(filename, 'w') as f:\n",
        "#     f.write(A_corpora[filename])\n",
        "\n",
        "# import pickle\n",
        "# with open('A_noising_map.pickle', 'wb') as f:\n",
        "#   pickle.dump(noising_map, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJT0Fcj3hZZo"
      },
      "source": [
        "## B. Compounding\n",
        "Only uses the top 5 percent bigrams found from wikipedia 100K dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqQ4apdgxvx"
      },
      "outputs": [],
      "source": [
        "# Compounder Code\n",
        "import random\n",
        "\n",
        "class CompoundNoise:\n",
        "\tdef __init__(self):\n",
        "\t\tself.mapping = {}\n",
        "\t\tself.reverse_mapping = {}\n",
        "\t\tself.banned = []\n",
        "\n",
        "\tdef set_map(self, mapping):\n",
        "\t\tself.mapping = mapping\n",
        "\n",
        "\tdef set_reverse_map(self, reverse_mapping):\n",
        "\t\tself.reverse_mapping = reverse_mapping\n",
        "\n",
        "\tdef set_banned(self, banned):\n",
        "\t\tself.banned = banned\n",
        "\n",
        "\tdef clear_map(self):\n",
        "\t\tself.mapping = {}\n",
        "\t\tself.reverse_mapping = {}\n",
        "\n",
        "\tdef get_map(self):\n",
        "\t\treturn self.mapping\n",
        "\n",
        "\tdef get_reverse_map(self):\n",
        "\t\treturn self.reverse_mapping\n",
        "\n",
        "\tdef get_banned(self):\n",
        "\t\treturn self.banned\n",
        "\n",
        "\tdef compound_token(self, s1, s2):\n",
        "\t\tif (s1, s2) in self.mapping:\n",
        "\t\t\treturn self.mapping[(s1, s2)]\n",
        "\n",
        "\t\t# try blending\n",
        "\t\tblended = self.generate_blend(s1, s2)\n",
        "\t\tif blended != -1:\n",
        "\t\t\tself.mapping[(s1, s2)] = blended\n",
        "\t\t\tself.reverse_mapping[blended] = (s1, s2)\n",
        "\t\t\treturn blended\n",
        "\n",
        "\t\tportmanteaued = self.generate_portmanteau(s1, s2)\n",
        "\t\tif portmanteaued != -1:\n",
        "\t\t\tself.mapping[(s1, s2)] = portmanteaued\n",
        "\t\t\tself.reverse_mapping[portmanteaued] = (s1, s2)\n",
        "\t\t\treturn portmanteaued\n",
        "\n",
        "\t\t# perform no compounding\n",
        "\t\treturn s1 + ' ' + s2\n",
        "\n",
        "\t# Helper function\n",
        "\tdef get_indices(self, lst, target_element):\n",
        "\t\treturn [index for index, element in enumerate(lst) if element == target_element]\n",
        "\n",
        "\tdef find_common_character(self, word1, word2):\n",
        "\t\tchars = []\n",
        "\t\tfor char in word1:\n",
        "\t\t\tif char in word2:\n",
        "\t\t\t\tchars.append(char)\n",
        "\t\treturn chars\n",
        "\n",
        "\tdef generate_blend(self, word1, word2):\n",
        "\t\tcommon_chars = list(set(self.find_common_character(word1, word2)))\n",
        "\t\tcandidates = []\n",
        "\t\tlen_diff = []\n",
        "\t\tfor common_char in common_chars:\n",
        "\t\t\tif common_char:\n",
        "\t\t\t\tindex1 = self.get_indices(word1, common_char)\n",
        "\t\t\t\tindex2 = self.get_indices(word2, common_char)\n",
        "\n",
        "\t\t\t\taverage_length = (len(word1) + len(word2))//2\n",
        "\t\t\t\tfor idx1 in index1:\n",
        "\t\t\t\t\tfor idx2 in index2:\n",
        "\t\t\t\t\t\tnew_word = word1[:idx1] + word2[idx2:]\n",
        "\t\t\t\t\t\tif new_word in self.mapping or new_word in self.banned:\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\tif abs(len(new_word) - average_length) <= 3:\n",
        "\t\t\t\t\t\t\tcandidates.append(new_word)\n",
        "\t\t\t\t\t\t\tlen_diff.append(abs(len(new_word) - average_length))\n",
        "\n",
        "\t\tmini = 999\n",
        "\t\tmini_idx = None\n",
        "\t\tfor i in range(len(candidates)):\n",
        "\t\t\tif len_diff[i] < mini:\n",
        "\t\t\t\tmini = len_diff[i]\n",
        "\t\t\t\tmini_idx = i\n",
        "\n",
        "\t\tif mini_idx is not None:\n",
        "\t\t\treturn candidates[mini_idx]\n",
        "\t\treturn -1\n",
        "\n",
        "\tdef generate_portmanteau(self, word1, word2):\n",
        "\t\tportmanteau_type = [1] * 2 + [2] * 1 + [3] * 2 + [4] * 5\n",
        "\t\trandom.shuffle(portmanteau_type)\n",
        "\n",
        "\t\tchoice = random.choice(portmanteau_type)\n",
        "\t\t# Type 1: Full Append (20% chance)\n",
        "\t\t# Example: [basket] + [ball] = basketball\n",
        "\t\tres = \"\"\n",
        "\t\tif choice == 1:\n",
        "\t\t\tres = word1 + word2\n",
        "\n",
        "\t\t# Type 2: word1 + half end of word2 (10% chance)\n",
        "\t\t# Example: [guess] + es[timate] = guesstimate\n",
        "\t\telif choice == 2:\n",
        "\t\t\tres = word1 + word2[int(len(word2)/2):]\n",
        "\n",
        "\t\t# Type 3: half first of word1 + half first of word2 (20% chance)\n",
        "\t\t# Example: [sit]uation + [com]edy = sitcom\n",
        "\t\telif choice == 3:\n",
        "\t\t\tres = word1[:int(len(word1)/2)] + word2[:int(len(word2)/2)]\n",
        "\n",
        "\t\t# Type 4: half first of word1 + half second of word2 (50% chance)\n",
        "\t\t# Example: [glam]orous + cam[ping] = glamping\n",
        "\t\telif choice == 4:\n",
        "\t\t\tres = word1[:int(len(word1)/2)] + word2[int(len(word2)/2):]\n",
        "\n",
        "\t\tif res in self.mapping or res in self.banned:\n",
        "\t\t\treturn -1\n",
        "\t\treturn res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb9PZ51BegPk"
      },
      "outputs": [],
      "source": [
        "Compounder = CompoundNoise()\n",
        "\n",
        "# Ban words that are already in the A_noising_map's keys\n",
        "import pickle\n",
        "mapping = {}\n",
        "with open(f'A_noising_map.pickle', 'rb') as f:\n",
        "  mapping = pickle.load(f)\n",
        "\n",
        "original = []\n",
        "for ori, trans in mapping.items():\n",
        "  original.append(ori)\n",
        "\n",
        "# Reminder, there are entries in original that translate into the same word\n",
        "BANNED = list(set(original))\n",
        "\n",
        "Compounder.set_banned(BANNED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BD454dRztQj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('word_bigram_lowercase_5plus.xlsx')\n",
        "df = df['bigram']\n",
        "\n",
        "bigram_list = df.tolist()\n",
        "bigram_list = [[word.split(',')[0][2:-1], word.split(',')[1][2:-2]] for word in bigram_list]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  Compounder.compound_token(bigram[0], bigram[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtQueLTQ_mZ8",
        "outputId": "680131d1-2a96-4b7e-ab63-16730b0481f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9861"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# before B_compounding\n",
        "len(Compounder.get_map())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mI1HhmR4IFS"
      },
      "outputs": [],
      "source": [
        "compound_map = Compounder.get_map()\n",
        "\n",
        "import pickle\n",
        "with open(f'B_compound_map.pickle', 'wb') as f:\n",
        "  pickle.dump(compound_map, f)\n",
        "\n",
        "with open(f'B_reverse_compound_map.pickle', 'wb') as f:\n",
        "  pickle.dump(Compounder.get_reverse_map(), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQeseRzT4U0Y"
      },
      "outputs": [],
      "source": [
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "B_corpora = {}\n",
        "\n",
        "def B_compounding(corpus, compound_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = sentence.split(' ')\n",
        "    if len(tokens) == 1:\n",
        "      return str(tokens[0])\n",
        "\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "\n",
        "    for bigram_idx in range(len(bigrams)):\n",
        "      if bigrams[bigram_idx] in compound_map:\n",
        "        bigrams[bigram_idx] = (compound_map[bigrams[bigram_idx]], '')\n",
        "\n",
        "        if bigram_idx == 0:\n",
        "          bigrams[1] = ('', bigrams[1][1])\n",
        "        elif bigram_idx == len(bigrams) - 1:\n",
        "          bigrams[len(bigrams) - 2] = (bigrams[len(bigrams) - 2][0], '')\n",
        "        else:\n",
        "          bigrams[bigram_idx-1] = (bigrams[bigram_idx-1][0], '')\n",
        "          bigrams[bigram_idx+1] = ('', bigrams[bigram_idx+1][1])\n",
        "\n",
        "    reconstruct = []\n",
        "    for i in range(len(bigrams)):\n",
        "      if i != len(bigrams) - 1:\n",
        "        if bigrams[i][0] != '':\n",
        "          reconstruct.append(bigrams[i][0])\n",
        "      else:\n",
        "        if bigrams[i][0] != '':\n",
        "          reconstruct.append(bigrams[i][0])\n",
        "          reconstruct.append(bigrams[i][1])\n",
        "        else:\n",
        "          reconstruct.append(bigrams[i][1])\n",
        "    reconstruct = ' '.join(reconstruct)\n",
        "    res += reconstruct\n",
        "\n",
        "  return res\n",
        "\n",
        "import pickle\n",
        "compound_map = {}\n",
        "with open('B_compound_map.pickle', 'rb') as f:\n",
        "  compound_map = pickle.load(f)\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(corpus, 'r') as f:\n",
        "    B_corpora[f'B_{corpus}'] = B_compounding(f.readlines(), compound_map)\n",
        "\n",
        "for filename in B_corpora.keys():\n",
        "  with open(f'temp_{filename}', 'w') as f:\n",
        "    f.write(B_corpora[filename])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC77d0XUARwW"
      },
      "source": [
        "## A + B\n",
        "1. Get B files\n",
        "2. Noise using A_mapping_original.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuLogMZXA2Um"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "import pickle\n",
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "A_noising_map = {}\n",
        "with open('A_noising_map.pickle', 'rb') as f:\n",
        "  A_noising_map = pickle.load(f)\n",
        "\n",
        "refs_file = [f\"B_{corpus}\" for corpus in corpora]\n",
        "AB_noising_map = obtain_mapping(refs_file, translation_dict, A_noising_map)\n",
        "\n",
        "AB_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'B_{corpus}', 'r') as f:\n",
        "    AB_corpora[f'AB_{corpus}'] = A_noising(f.readlines(), AB_noising_map)\n",
        "\n",
        "with open('AB_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(AB_noising_map, f)\n",
        "\n",
        "for filename in AB_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(AB_corpora[filename])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIPggPYKCtAf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNr7OgrNEk-y"
      },
      "source": [
        "## C/Dx\n",
        "Get mapping first, use google translate (Created on 22/09/2023) for the four following language:\n",
        "\n",
        "    C1. German (de)\n",
        "    C2. Portuguese (pt)\n",
        "    D1. Afrikaans (af)\n",
        "    D2. Galician (gl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtdcK7xEzRC"
      },
      "outputs": [],
      "source": [
        "!gcloud auth application-default login\n",
        "!gcloud auth application-default set-quota-project resolute-parity-392608\n",
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP1HlJaVFCFr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "def romanize_text(src, contents):\n",
        "    # Define the data you want to send in the POST request\n",
        "    data = {\n",
        "        'source_language_code': src,\n",
        "        'contents': contents\n",
        "    }\n",
        "\n",
        "    # Define the file path where you want to save the JSON data\n",
        "    file_path = 'request.json'\n",
        "\n",
        "    # Write the dictionary to the JSON file\n",
        "    with open(file_path, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    command = \"\"\"\n",
        "    curl -X POST \\\n",
        "        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "        -H \"x-goog-user-project: resolute-parity-392608\" \\\n",
        "        -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "        -d @request.json \\\n",
        "        \"https://translation.googleapis.com/v3/projects/resolute-parity-392608/locations/us-central1:romanizeText\" \\\n",
        "        -o \"romanized.json\"\n",
        "    \"\"\"\n",
        "    subprocess.run(command, shell=True, text=True, stdout=subprocess.PIPE)\n",
        "\n",
        "    romanized_text_list = []\n",
        "    with open('romanized.json', 'r') as f:\n",
        "      romanized_texts = dict(json.load(f))['romanizations']\n",
        "\n",
        "      for roman_text in romanized_texts:\n",
        "        romanized_text_list.append(roman_text['romanizedText'])\n",
        "\n",
        "    return romanized_text_list\n",
        "\n",
        "# romanize_text('ja', ['お元気ですか', '私は元気です、あなたはどうですか'])\n",
        "\n",
        "def translate_text(target: str, source: str, text: str) -> dict:\n",
        "    \"\"\"Translates text into the target language.\n",
        "\n",
        "    Target must be an ISO 639-1 language code.\n",
        "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
        "    \"\"\"\n",
        "    from google.cloud import translate_v2 as translate\n",
        "\n",
        "    translate_client = translate.Client()\n",
        "\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode(\"utf-8\")\n",
        "\n",
        "    result = translate_client.translate(text, target_language=target, source_language=source)\n",
        "\n",
        "    non_roman = {\n",
        "      'ar': 'Arabic',\n",
        "      'am': 'Amharic',\n",
        "      'bn': 'Bengali',\n",
        "      'be': 'Belarusian',\n",
        "      'hi': 'Hindi',\n",
        "      'ja': 'Japanese',\n",
        "      'my': 'Myanmar',\n",
        "      'ru': 'Russian',\n",
        "      'sr': 'Serbian',\n",
        "      'uk': 'Ukrainian'\n",
        "    }\n",
        "\n",
        "    if target in non_roman.keys():\n",
        "      result = romanize_text(target, result['translatedText'])\n",
        "      return result\n",
        "    return result['translatedText']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9k1WzUnFMln"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "en_words = []\n",
        "en_dicts = {}\n",
        "\n",
        "with open('A_noising_map.pickle', 'rb') as f:\n",
        "  en_dicts = pickle.load(f)\n",
        "\n",
        "en_words = list(en_dicts.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uIIvtMxFcmD"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "\n",
        "prob = 0.2 # 20% of the dictionary will be cognated\n",
        "flag_translate = [1 if random.random() < prob else 0 for i in range(len(en_words))]\n",
        "\n",
        "i = 0\n",
        "translate_map = {}\n",
        "for word in en_words:\n",
        "  translate_map[word] = flag_translate[i]\n",
        "  i += 1\n",
        "\n",
        "C1_mapping = {}\n",
        "C2_mapping = {}\n",
        "D1_mapping = {}\n",
        "D2_mapping = {}\n",
        "\n",
        "languages = ['de', 'pt', 'af', 'gl']\n",
        "\n",
        "def transform_map(translate_map, tgt):\n",
        "  result_map = {}\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      result_map[word] = translate_text(tgt, 'en', word)\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "  return result_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2NqtVePI07W"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('translate_map.pickle', 'wb') as f:\n",
        "  pickle.dump(translate_map, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oHzGJRUyGcxf",
        "outputId": "7b3ae9aa-f47a-4ab7-a143-269a82832824"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10608/10608 [42:41<00:00,  4.14it/s]\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9fddb55e-cde2-41dd-9033-0c87c54366ca\", \"C1_mapping.pickle\", 170394)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "C1_mapping = transform_map(translate_map, 'de')\n",
        "with open('C1_mapping.pickle', 'wb') as file:\n",
        "    pickle.dump(C1_mapping, file)\n",
        "files.download('C1_mapping.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rf0FTCglGldl",
        "outputId": "d6d0b897-635c-4a8c-ddbd-256204371ac5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10608/10608 [41:58<00:00,  4.21it/s]\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_7c651ef1-d6cf-4975-8cc3-3a29bc617c98\", \"C2_mapping.pickle\", 170525)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "C2_mapping = transform_map(translate_map, 'pt')\n",
        "with open('C2_mapping.pickle', 'wb') as file:\n",
        "    pickle.dump(C2_mapping, file)\n",
        "files.download('C2_mapping.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JqcoYAOkGq9y",
        "outputId": "fe365713-2fc3-4cc5-c76a-6c5c4f2bd52a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10608/10608 [41:33<00:00,  4.25it/s]\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_6e0b53f2-2a73-428e-ab80-eff6a69e3940\", \"D1_mapping.pickle\", 169090)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "D1_mapping = transform_map(translate_map, 'af')\n",
        "with open('D1_mapping.pickle', 'wb') as file:\n",
        "    pickle.dump(D1_mapping, file)\n",
        "files.download('D1_mapping.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nso2rG3lGxqe",
        "outputId": "1a4743cb-d2da-46cb-b0fc-e3c6539d24fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10608/10608 [42:46<00:00,  4.13it/s]\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_2e6e33ea-3018-4b1b-9636-f733473475ce\", \"D2_mapping.pickle\", 170210)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "D2_mapping = transform_map(translate_map, 'gl')\n",
        "with open('D2_mapping.pickle', 'wb') as file:\n",
        "    pickle.dump(D2_mapping, file)\n",
        "files.download('D2_mapping.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayDQuqKuFnmh"
      },
      "source": [
        "### Mapping and Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RWwR9XbRO8T"
      },
      "outputs": [],
      "source": [
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "import pickle\n",
        "with open(f'C1_mapping.pickle', 'rb') as f:\n",
        "  C1_map = pickle.load(f)\n",
        "\n",
        "with open(f'C2_mapping.pickle', 'rb') as f:\n",
        "  C2_map = pickle.load(f)\n",
        "\n",
        "with open(f'D1_mapping.pickle', 'rb') as f:\n",
        "  D1_map = pickle.load(f)\n",
        "\n",
        "with open(f'D2_mapping.pickle', 'rb') as f:\n",
        "  D2_map = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDSWfRplFnI4"
      },
      "outputs": [],
      "source": [
        "def translate_corpora(code, files, mapping):\n",
        "  for file_ in files:\n",
        "    res = ''\n",
        "    with open(file_, 'r') as f:\n",
        "      for sentence in f:\n",
        "        tokens = tokenize(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "          tokens[i] = mapping.get(tokens[i], tokens[i])\n",
        "\n",
        "        corrupted_sentence = recombine(tokens)\n",
        "        res += corrupted_sentence +'\\n'\n",
        "\n",
        "    with open(f\"{code}_{file_}\", 'w') as f:\n",
        "      f.write(res)\n",
        "\n",
        "translate_corpora('C1', corpora, C1_map)\n",
        "translate_corpora('C2', corpora, C2_map)\n",
        "translate_corpora('D1', corpora, D1_map)\n",
        "translate_corpora('D2', corpora, D2_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sNd7pxmRRym"
      },
      "source": [
        "## (C/D + A). Cognates with noising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMVkKPEqiPkl"
      },
      "source": [
        "### A. Function:\n",
        "1. A_noising\n",
        "2. obtain_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52dr3IOUiZXE"
      },
      "outputs": [],
      "source": [
        "def obtain_mapping(refs_file, translation_dict, default_noising_map = {}, allow_update = False):\n",
        "  def tokenize(text):\n",
        "    import re\n",
        "    import string\n",
        "    ret = []\n",
        "    for token in text.split(' '):\n",
        "      result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "      flag = 0\n",
        "      for token in result_list:\n",
        "        if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "          flag = 1\n",
        "      if flag:\n",
        "        ret.append(''.join(result_list))\n",
        "      elif len(result_list) > 2:\n",
        "        ret.append(''.join(result_list))\n",
        "      else:\n",
        "        ret.extend(result_list)\n",
        "    return ret\n",
        "\n",
        "  noising_map = default_noising_map\n",
        "  for ref_file in refs_file:\n",
        "    with open(ref_file, 'r') as f:\n",
        "      for sentence in f.readlines():\n",
        "        tokens = tokenize(sentence)\n",
        "        split_2 = [[token[i:i+2] for i in range(0, len(token), 2)] for token in tokens]\n",
        "        for split_idx in range(len(split_2)):\n",
        "          split = split_2[split_idx]\n",
        "          for i in range(len(split)):\n",
        "            if split[i] in translation_dict:\n",
        "              split[i] = translation_dict[split[i]]\n",
        "\n",
        "        corrupted_tokens = [''.join(subtokens) for subtokens in split_2]\n",
        "        for ori, trans in zip(tokens, corrupted_tokens):\n",
        "          if noising_map.get(ori, 0) == 0 or noising_map.get(ori, 0) == trans:\n",
        "            noising_map[ori] = trans\n",
        "          elif noising_map[ori] != trans and allow_update:\n",
        "            noising_map[ori] = trans\n",
        "          else:\n",
        "            print(f\"noising_map[{ori}] = {noising_map[ori]}, not {trans} | However, update is not allowed\")\n",
        "  return noising_map\n",
        "\n",
        "def A_noising(corpus, noising_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = tokenize(sentence)\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = noising_map[tokens[i]]\n",
        "\n",
        "    corrupted_sentence = recombine(tokens)\n",
        "    res += corrupted_sentence +'\\n'\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXuQtgGZpFqT"
      },
      "outputs": [],
      "source": [
        "def translate_corpora(code, files, mapping):\n",
        "  for file_ in files:\n",
        "    res = ''\n",
        "    with open(file_, 'r') as f:\n",
        "      for sentence in f:\n",
        "        tokens = tokenize(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "          tokens[i] = mapping[tokens[i]]\n",
        "\n",
        "        corrupted_sentence = recombine(tokens)\n",
        "        res += corrupted_sentence +'\\n'\n",
        "\n",
        "    with open(f\"{code}_{file_}\", 'w') as f:\n",
        "      f.write(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNZTAhui26C"
      },
      "source": [
        "### C1 + A. German Cognated + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRLpKMCtjTiP"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "translate_corpora('C1', corpora, C1_map)\n",
        "\n",
        "C1_noise_map = {}\n",
        "with open('C1_mapping.pickle', 'rb') as f:\n",
        "  C1_noise_map = pickle.load(f) # Basically, C1_noise_map = C1_map\n",
        "\n",
        "refs_file = [f\"C1_{corpus}\" for corpus in corpora]\n",
        "C1A_noising_map = obtain_mapping(refs_file, translation_dict, C1_noise_map, allow_update = True)\n",
        "\n",
        "C1A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'C1_{corpus}', 'r') as f:\n",
        "    C1A_corpora[f'C1A_{corpus}'] = A_noising(f.readlines(), C1A_noising_map)\n",
        "\n",
        "with open('C1A_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(C1A_noising_map, f)\n",
        "\n",
        "for filename in C1A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(C1A_corpora[filename])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNo7MB9qv_yi"
      },
      "outputs": [],
      "source": [
        "C1A_noising_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKs0JGsnjEkC"
      },
      "source": [
        "### C2 + A. Portuguese Cognated + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX1731xN1FSw"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "translate_corpora('C2', corpora, C2_map)\n",
        "\n",
        "C2_noise_map = {}\n",
        "with open('C2_mapping.pickle', 'rb') as f:\n",
        "  C2_noise_map = pickle.load(f)\n",
        "\n",
        "refs_file = [f\"C2_{corpus}\" for corpus in corpora]\n",
        "C2A_noising_map = obtain_mapping(refs_file, translation_dict, C2_noise_map, allow_update = True)\n",
        "\n",
        "C2A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'C2_{corpus}', 'r') as f:\n",
        "    C2A_corpora[f'C2A_{corpus}'] = A_noising(f.readlines(), C2A_noising_map)\n",
        "\n",
        "with open('C2A_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(C2A_noising_map, f)\n",
        "\n",
        "for filename in C2A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(C2A_corpora[filename])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaCM_-1rjIIn"
      },
      "source": [
        "### D1 + A. Afrikaans Cognated + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxftDMnb1h7m"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "translate_corpora('D1', corpora, D1_map)\n",
        "\n",
        "D1_noise_map = {}\n",
        "with open('D1_mapping.pickle', 'rb') as f:\n",
        "  D1_noise_map = pickle.load(f)\n",
        "\n",
        "refs_file = [f\"D1_{corpus}\" for corpus in corpora]\n",
        "D1A_noising_map = obtain_mapping(refs_file, translation_dict, D1_noise_map, allow_update = True)\n",
        "\n",
        "D1A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'D1_{corpus}', 'r') as f:\n",
        "    D1A_corpora[f'D1A_{corpus}'] = A_noising(f.readlines(), D1A_noising_map)\n",
        "\n",
        "with open('D1A_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(D1A_noising_map, f)\n",
        "\n",
        "for filename in D1A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(D1A_corpora[filename])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FEeODwjK7p"
      },
      "source": [
        "### D2 + A. Galician Cognated + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPfQqGeo2Fdd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "translate_corpora('D2', corpora, D1_map)\n",
        "\n",
        "D2_noise_map = {}\n",
        "with open('D2_mapping.pickle', 'rb') as f:\n",
        "  D2_noise_map = pickle.load(f)\n",
        "\n",
        "refs_file = [f\"D2_{corpus}\" for corpus in corpora]\n",
        "D2A_noising_map = obtain_mapping(refs_file, translation_dict, D2_noise_map, allow_update = True)\n",
        "\n",
        "D2A_corpora = {}\n",
        "for corpus in corpora:\n",
        "  with open(f'D2_{corpus}', 'r') as f:\n",
        "    D2A_corpora[f'D2A_{corpus}'] = A_noising(f.readlines(), D2A_noising_map)\n",
        "\n",
        "with open('D2A_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(D2A_noising_map, f)\n",
        "\n",
        "for filename in D2A_corpora.keys():\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(D2A_corpora[filename])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCCd8xhY2-1M"
      },
      "source": [
        "## (C/D + B). Cognates with Compounding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0f6uJ9O3Oqf"
      },
      "source": [
        "### B. Function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEkqBsag5SCP"
      },
      "outputs": [],
      "source": [
        "# Compounder Code\n",
        "import random\n",
        "\n",
        "class CompoundNoise:\n",
        "\tdef __init__(self):\n",
        "\t\tself.mapping = {}\n",
        "\t\tself.reverse_mapping = {}\n",
        "\t\tself.banned = []\n",
        "\n",
        "\tdef set_map(self, mapping):\n",
        "\t\tself.mapping = mapping\n",
        "\n",
        "\tdef set_reverse_map(self, reverse_mapping):\n",
        "\t\tself.reverse_mapping = reverse_mapping\n",
        "\n",
        "\tdef set_banned(self, banned):\n",
        "\t\tself.banned = banned\n",
        "\n",
        "\tdef clear_map(self):\n",
        "\t\tself.mapping = {}\n",
        "\t\tself.reverse_mapping = {}\n",
        "\n",
        "\tdef get_map(self):\n",
        "\t\treturn self.mapping\n",
        "\n",
        "\tdef get_reverse_map(self):\n",
        "\t\treturn self.reverse_mapping\n",
        "\n",
        "\tdef get_banned(self):\n",
        "\t\treturn self.banned\n",
        "\n",
        "\tdef compound_token(self, s1, s2):\n",
        "\t\tif (s1, s2) in self.mapping:\n",
        "\t\t\treturn self.mapping[(s1, s2)]\n",
        "\n",
        "\t\t# try blending\n",
        "\t\tblended = self.generate_blend(s1, s2)\n",
        "\t\tif blended != -1:\n",
        "\t\t\tself.mapping[(s1, s2)] = blended\n",
        "\t\t\tself.reverse_mapping[blended] = (s1, s2)\n",
        "\t\t\treturn blended\n",
        "\n",
        "\t\tportmanteaued = self.generate_portmanteau(s1, s2)\n",
        "\t\tif portmanteaued != -1:\n",
        "\t\t\tself.mapping[(s1, s2)] = portmanteaued\n",
        "\t\t\tself.reverse_mapping[portmanteaued] = (s1, s2)\n",
        "\t\t\treturn portmanteaued\n",
        "\n",
        "\t\t# perform no compounding\n",
        "\t\treturn s1 + ' ' + s2\n",
        "\n",
        "\t# Helper function\n",
        "\tdef get_indices(self, lst, target_element):\n",
        "\t\treturn [index for index, element in enumerate(lst) if element == target_element]\n",
        "\n",
        "\tdef find_common_character(self, word1, word2):\n",
        "\t\tchars = []\n",
        "\t\tfor char in word1:\n",
        "\t\t\tif char in word2:\n",
        "\t\t\t\tchars.append(char)\n",
        "\t\treturn chars\n",
        "\n",
        "\tdef generate_blend(self, word1, word2):\n",
        "\t\tcommon_chars = list(set(self.find_common_character(word1, word2)))\n",
        "\t\tcandidates = []\n",
        "\t\tlen_diff = []\n",
        "\t\tfor common_char in common_chars:\n",
        "\t\t\tif common_char:\n",
        "\t\t\t\tindex1 = self.get_indices(word1, common_char)\n",
        "\t\t\t\tindex2 = self.get_indices(word2, common_char)\n",
        "\n",
        "\t\t\t\taverage_length = (len(word1) + len(word2))//2\n",
        "\t\t\t\tfor idx1 in index1:\n",
        "\t\t\t\t\tfor idx2 in index2:\n",
        "\t\t\t\t\t\tnew_word = word1[:idx1] + word2[idx2:]\n",
        "\t\t\t\t\t\tif new_word in self.mapping or new_word in self.banned:\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\tif abs(len(new_word) - average_length) <= 3:\n",
        "\t\t\t\t\t\t\tcandidates.append(new_word)\n",
        "\t\t\t\t\t\t\tlen_diff.append(abs(len(new_word) - average_length))\n",
        "\n",
        "\t\tmini = 999\n",
        "\t\tmini_idx = None\n",
        "\t\tfor i in range(len(candidates)):\n",
        "\t\t\tif len_diff[i] < mini:\n",
        "\t\t\t\tmini = len_diff[i]\n",
        "\t\t\t\tmini_idx = i\n",
        "\n",
        "\t\tif mini_idx is not None:\n",
        "\t\t\treturn candidates[mini_idx]\n",
        "\t\treturn -1\n",
        "\n",
        "\tdef generate_portmanteau(self, word1, word2):\n",
        "\t\tportmanteau_type = [1] * 2 + [2] * 1 + [3] * 2 + [4] * 5\n",
        "\t\trandom.shuffle(portmanteau_type)\n",
        "\n",
        "\t\tchoice = random.choice(portmanteau_type)\n",
        "\t\t# Type 1: Full Append (20% chance)\n",
        "\t\t# Example: [basket] + [ball] = basketball\n",
        "\t\tres = \"\"\n",
        "\t\tif choice == 1:\n",
        "\t\t\tres = word1 + word2\n",
        "\n",
        "\t\t# Type 2: word1 + half end of word2 (10% chance)\n",
        "\t\t# Example: [guess] + es[timate] = guesstimate\n",
        "\t\telif choice == 2:\n",
        "\t\t\tres = word1 + word2[int(len(word2)/2):]\n",
        "\n",
        "\t\t# Type 3: half first of word1 + half first of word2 (20% chance)\n",
        "\t\t# Example: [sit]uation + [com]edy = sitcom\n",
        "\t\telif choice == 3:\n",
        "\t\t\tres = word1[:int(len(word1)/2)] + word2[:int(len(word2)/2)]\n",
        "\n",
        "\t\t# Type 4: half first of word1 + half second of word2 (50% chance)\n",
        "\t\t# Example: [glam]orous + cam[ping] = glamping\n",
        "\t\telif choice == 4:\n",
        "\t\t\tres = word1[:int(len(word1)/2)] + word2[int(len(word2)/2):]\n",
        "\n",
        "\t\tif res in self.mapping or res in self.banned:\n",
        "\t\t\treturn -1\n",
        "\t\treturn res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aFjla9-9cPg"
      },
      "outputs": [],
      "source": [
        "def B_compounding(corpus, compound_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = sentence.split(' ')\n",
        "    if len(tokens) == 1:\n",
        "      return str(tokens[0])\n",
        "\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "\n",
        "    for bigram_idx in range(len(bigrams)):\n",
        "      if bigrams[bigram_idx] in compound_map:\n",
        "        bigrams[bigram_idx] = (compound_map[bigrams[bigram_idx]], '')\n",
        "\n",
        "        if bigram_idx == 0:\n",
        "          bigrams[1] = ('', bigrams[1][1])\n",
        "        elif bigram_idx == len(bigrams) - 1:\n",
        "          bigrams[len(bigrams) - 2] = (bigrams[len(bigrams) - 2][0], '')\n",
        "        else:\n",
        "          bigrams[bigram_idx-1] = (bigrams[bigram_idx-1][0], '')\n",
        "          bigrams[bigram_idx+1] = ('', bigrams[bigram_idx+1][1])\n",
        "\n",
        "    reconstruct = []\n",
        "    for i in range(len(bigrams)):\n",
        "      if i != len(bigrams) - 1:\n",
        "        if bigrams[i][0] != '':\n",
        "          reconstruct.append(bigrams[i][0])\n",
        "      else:\n",
        "        if bigrams[i][0] != '':\n",
        "          reconstruct.append(bigrams[i][0])\n",
        "          reconstruct.append(bigrams[i][1])\n",
        "        else:\n",
        "          reconstruct.append(bigrams[i][1])\n",
        "    reconstruct = ' '.join(reconstruct)\n",
        "    res += reconstruct\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAw82d893Q9n"
      },
      "source": [
        "### C1 + B German Cognates + Compounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9vdUeQL5gD7"
      },
      "outputs": [],
      "source": [
        "Compounder = CompoundNoise()\n",
        "\n",
        "# Ban words that are already in the A_noising_map's keys\n",
        "import pickle\n",
        "mapping = {}\n",
        "with open(f'C1_mapping.pickle', 'rb') as f:\n",
        "  mapping = pickle.load(f)\n",
        "\n",
        "# Because of Cognates, we do not want the Compounding resulting in an already existing\n",
        "# word in either english or the other language\n",
        "original = []\n",
        "for ori, trans in mapping.items():\n",
        "  original.append(ori)\n",
        "  original.append(trans)\n",
        "\n",
        "# Reminder, there are entries in original that translate into the same word\n",
        "BANNED = list(set(original))\n",
        "\n",
        "Compounder.set_banned(BANNED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_lyw30YDDQj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('word_bigram_lowercase_5plus.xlsx')\n",
        "df = df['bigram']\n",
        "\n",
        "bigram_list = df.tolist()\n",
        "bigram_list = [[word.split(',')[0][2:-1], word.split(',')[1][2:-2]] for word in bigram_list]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  for i in range(len(bigram)):\n",
        "    if bigram[i] in C1_map:\n",
        "      bigram[i] = C1_map[bigram[i]]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  Compounder.compound_token(bigram[0], bigram[1])\n",
        "\n",
        "compound_map_C1 = Compounder.get_map()\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'C1_{corpus}', 'r') as f, open(f'C1B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), compound_map_C1))\n",
        "\n",
        "import pickle\n",
        "with open(f'C1B_compound_map.pickle', 'wb') as f:\n",
        "  pickle.dump(compound_map_C1, f)\n",
        "\n",
        "with open(f'C1B_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**compound_map_C1, **C1_map} ,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNmRzc-W3VjD"
      },
      "source": [
        "### C2 + B Portuguese Cognates + Compounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am0r9iZG_Nvd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('word_bigram_lowercase_5plus.xlsx')\n",
        "df = df['bigram']\n",
        "\n",
        "bigram_list = df.tolist()\n",
        "bigram_list = [[word.split(',')[0][2:-1], word.split(',')[1][2:-2]] for word in bigram_list]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  for i in range(len(bigram)):\n",
        "    if bigram[i] in C2_map:\n",
        "      bigram[i] = C2_map[bigram[i]]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  Compounder.compound_token(bigram[0], bigram[1])\n",
        "\n",
        "compound_map_C2 = Compounder.get_map()\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'C2_{corpus}', 'r') as f, open(f'C2B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), compound_map_C2))\n",
        "\n",
        "import pickle\n",
        "with open(f'C2B_compound_map.pickle', 'wb') as f:\n",
        "  pickle.dump(compound_map_C2, f)\n",
        "\n",
        "with open(f'C2B_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**compound_map_C2, **C2_map} ,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJZitF_C3YWT"
      },
      "source": [
        "### D1 + B Afrikaans Cognates + Compounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77zsgwOl_YU6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('word_bigram_lowercase_5plus.xlsx')\n",
        "df = df['bigram']\n",
        "\n",
        "bigram_list = df.tolist()\n",
        "bigram_list = [[word.split(',')[0][2:-1], word.split(',')[1][2:-2]] for word in bigram_list]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  for i in range(len(bigram)):\n",
        "    if bigram[i] in D1_map:\n",
        "      bigram[i] = D1_map[bigram[i]]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  Compounder.compound_token(bigram[0], bigram[1])\n",
        "\n",
        "compound_map_D1 = Compounder.get_map()\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'D1_{corpus}', 'r') as f, open(f'D1B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), compound_map_D1))\n",
        "\n",
        "import pickle\n",
        "with open(f'D1B_compound_map.pickle', 'wb') as f:\n",
        "  pickle.dump(compound_map_D1, f)\n",
        "\n",
        "with open(f'D1B_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**compound_map_D1, **D1_map} ,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIT_bfJW3bbH"
      },
      "source": [
        "### D2 + B Galician Cognates + Compounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG4KktVJ_hKk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('word_bigram_lowercase_5plus.xlsx')\n",
        "df = df['bigram']\n",
        "\n",
        "bigram_list = df.tolist()\n",
        "bigram_list = [[word.split(',')[0][2:-1], word.split(',')[1][2:-2]] for word in bigram_list]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  for i in range(len(bigram)):\n",
        "    if bigram[i] in D2_map:\n",
        "      bigram[i] = D2_map[bigram[i]]\n",
        "\n",
        "for bigram in bigram_list:\n",
        "  Compounder.compound_token(bigram[0], bigram[1])\n",
        "\n",
        "compound_map_D2 = Compounder.get_map()\n",
        "\n",
        "for corpus in corpora:\n",
        "  with open(f'D2_{corpus}', 'r') as f, open(f'D2B_{corpus}', 'w') as g:\n",
        "    g.write(B_compounding(f.readlines(), compound_map_D2))\n",
        "\n",
        "import pickle\n",
        "with open(f'D2B_compound_map.pickle', 'wb') as f:\n",
        "  pickle.dump(compound_map_D2, f)\n",
        "\n",
        "with open(f'D2B_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**compound_map_D2, **D2_map} ,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjMlM2Q2BrJL"
      },
      "source": [
        "## (C/D + B + A). Cognates with Compounding THEN Noising\n",
        "\n",
        "> From the results of C/D + B, get **C/Dx_noising_map**. Use it as default mapping for obtain_mapping function. Allow updates = True, save the mapping in **C/Dx_BA_noising_map.pickle**. Noise the C/D + B corpus.\n",
        "\n",
        "> This means: We utlized **C/Dx_map** to obtain **C/DxA_noising_map.pickle**, which we then use to obtain **C/DxBA_noising_map.pickle**.\n",
        "\n",
        "> We also need **C/Dx_compound_map.pickle **(NOT COMBINED_MAP) to then reconstruct English corpus into (C/D + B) corpus first. To make things easier, we can create **C/DxBA_combined_map.pickle**, consisting of **C/Dx_compound_map.pickle + C/DxBA_noising_map.pickle.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_R9SJdfDNbi"
      },
      "source": [
        "### BA. Function:\n",
        "--> Actually, we only need the A functions since this is done after (C/D + B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcYrvnKVDMol"
      },
      "outputs": [],
      "source": [
        "def obtain_mapping(refs_file, translation_dict, default_noising_map = {}, allow_update = False):\n",
        "  def tokenize(text):\n",
        "    import re\n",
        "    import string\n",
        "    ret = []\n",
        "    for token in text.split(' '):\n",
        "      result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "      flag = 0\n",
        "      for token in result_list:\n",
        "        if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "          flag = 1\n",
        "      if flag:\n",
        "        ret.append(''.join(result_list))\n",
        "      elif len(result_list) > 2:\n",
        "        ret.append(''.join(result_list))\n",
        "      else:\n",
        "        ret.extend(result_list)\n",
        "    return ret\n",
        "\n",
        "  noising_map = default_noising_map\n",
        "  for ref_file in refs_file:\n",
        "    with open(ref_file, 'r') as f:\n",
        "      for sentence in f.readlines():\n",
        "        tokens = tokenize(sentence)\n",
        "        split_2 = [[token[i:i+2] for i in range(0, len(token), 2)] for token in tokens]\n",
        "        for split_idx in range(len(split_2)):\n",
        "          split = split_2[split_idx]\n",
        "          for i in range(len(split)):\n",
        "            if split[i] in translation_dict:\n",
        "              split[i] = translation_dict[split[i]]\n",
        "\n",
        "        corrupted_tokens = [''.join(subtokens) for subtokens in split_2]\n",
        "        for ori, trans in zip(tokens, corrupted_tokens):\n",
        "          if noising_map.get(ori, 0) == 0 or noising_map.get(ori, 0) == trans:\n",
        "            noising_map[ori] = trans\n",
        "          elif noising_map[ori] != trans and allow_update:\n",
        "            noising_map[ori] = trans\n",
        "          else:\n",
        "            print(f\"noising_map[{ori}] = {noising_map[ori]}, not {trans} | However, update is not allowed\")\n",
        "  return noising_map\n",
        "\n",
        "def A_noising(corpus, noising_map):\n",
        "  res = ''\n",
        "  for sentence in corpus:\n",
        "    tokens = tokenize(sentence)\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = noising_map[tokens[i]]\n",
        "\n",
        "    corrupted_sentence = recombine(tokens)\n",
        "    res += corrupted_sentence +'\\n'\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewFCDIFLEgar"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g45WynJfEjR7"
      },
      "outputs": [],
      "source": [
        "corpora = [\n",
        "    'flores_dev_english_sov', 'flores_dev_english_vos', 'flores_dev_english_vso', 'flores_dev_english_svo',\n",
        "    'flores_english_sov', 'flores_english_vos', 'flores_english_vso', 'flores_english_svo'\n",
        "]\n",
        "\n",
        "import pickle\n",
        "\n",
        "C1A_noising_map = {}\n",
        "with open('C1A_noising_map.pickle', 'rb') as f:\n",
        "  C1A_noising_map = pickle.load(f)\n",
        "\n",
        "C2A_noising_map = {}\n",
        "with open('C2A_noising_map.pickle', 'rb') as f:\n",
        "  C2A_noising_map = pickle.load(f)\n",
        "\n",
        "D1A_noising_map = {}\n",
        "with open('D1A_noising_map.pickle', 'rb') as f:\n",
        "  D1A_noising_map = pickle.load(f)\n",
        "\n",
        "D2A_noising_map = {}\n",
        "with open('D2A_noising_map.pickle', 'rb') as f:\n",
        "  D2A_noising_map = pickle.load(f)\n",
        "\n",
        "import pandas\n",
        "df = pd.read_excel('bigram_5p.xlsx')\n",
        "translation_dict = df.set_index('Original')['Translation'].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OBFSBGSHHfJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def replace_full_words(text, old_word, new_word):\n",
        "    pattern = r'\\b{}\\b'.format(re.escape(old_word))\n",
        "    result = re.sub(pattern, new_word, text)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL0scCmxDWPN"
      },
      "source": [
        "### C1 + B + A. German Cognates + Compounding + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsCk668yEaQ4"
      },
      "outputs": [],
      "source": [
        "refs_file = [f\"C1B_{corpus}\" for corpus in corpora]\n",
        "C1BA_noising_map = obtain_mapping(refs_file, translation_dict, C1A_noising_map, allow_update = True)\n",
        "\n",
        "for corpus in corpora:\n",
        "  text = ''\n",
        "  with open(f'C1B_{corpus}', 'r') as f:\n",
        "    text = f.read()\n",
        "    for clean, noise in C1BA_noising_map.items():\n",
        "      if clean != noise:\n",
        "        text = replace_full_words(text, clean, noise)\n",
        "\n",
        "  with open(f'C1BA_{corpus}', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "import pickle\n",
        "C1B_compound_map = {}\n",
        "with open('C1B_compound_map.pickle', 'rb') as f:\n",
        "  C1B_compound_map = pickle.load(f)\n",
        "\n",
        "with open('C1BA_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(C1BA_noising_map, f)\n",
        "\n",
        "with open('C1BA_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**C1B_compound_map, **C1BA_noising_map}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA0cPrwXDagy"
      },
      "source": [
        "### C2 + B + A. Portuguese Cognates + Compounding + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COC-WOoeLyWZ"
      },
      "outputs": [],
      "source": [
        "refs_file = [f\"C2B_{corpus}\" for corpus in corpora]\n",
        "C2BA_noising_map = obtain_mapping(refs_file, translation_dict, C2A_noising_map, allow_update = True)\n",
        "\n",
        "for corpus in corpora:\n",
        "  text = ''\n",
        "  with open(f'C2B_{corpus}', 'r') as f:\n",
        "    text = f.read()\n",
        "    for clean, noise in C2BA_noising_map.items():\n",
        "      if clean != noise:\n",
        "        text = replace_full_words(text, clean, noise)\n",
        "\n",
        "  with open(f'C2BA_{corpus}', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "import pickle\n",
        "C2B_compound_map = {}\n",
        "with open('C2B_compound_map.pickle', 'rb') as f:\n",
        "  C2B_compound_map = pickle.load(f)\n",
        "\n",
        "with open('C2BA_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(C2BA_noising_map, f)\n",
        "\n",
        "with open('C2BA_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**C2B_compound_map, **C2BA_noising_map}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ad0j3KaDc_C"
      },
      "source": [
        "### D1 + B + A. Afrikaans Cognates + Compounding + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocu8CQ41NR1w"
      },
      "outputs": [],
      "source": [
        "refs_file = [f\"D1B_{corpus}\" for corpus in corpora]\n",
        "D1BA_noising_map = obtain_mapping(refs_file, translation_dict, D1A_noising_map, allow_update = True)\n",
        "\n",
        "for corpus in corpora:\n",
        "  text = ''\n",
        "  with open(f'D1B_{corpus}', 'r') as f:\n",
        "    text = f.read()\n",
        "    for clean, noise in D1BA_noising_map.items():\n",
        "      if clean != noise:\n",
        "        text = replace_full_words(text, clean, noise)\n",
        "\n",
        "  with open(f'D1BA_{corpus}', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "import pickle\n",
        "D1B_compound_map = {}\n",
        "with open('D1B_compound_map.pickle', 'rb') as f:\n",
        "  D1B_compound_map = pickle.load(f)\n",
        "\n",
        "with open('D1BA_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(D1BA_noising_map, f)\n",
        "\n",
        "with open('D1BA_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**D1B_compound_map, **D1BA_noising_map}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVfHlGBaDfsh"
      },
      "source": [
        "### D2 + B + A. Galician Cognates + Compounding + Noising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DQ_-IEZyDF63"
      },
      "outputs": [],
      "source": [
        "refs_file = [f\"D2B_{corpus}\" for corpus in corpora]\n",
        "D2BA_noising_map = obtain_mapping(refs_file, translation_dict, D2A_noising_map, allow_update = True)\n",
        "\n",
        "for corpus in corpora:\n",
        "  text = ''\n",
        "  with open(f'D2B_{corpus}', 'r') as f:\n",
        "    text = f.read()\n",
        "    for clean, noise in D2BA_noising_map.items():\n",
        "      if clean != noise:\n",
        "        text = replace_full_words(text, clean, noise)\n",
        "\n",
        "  with open(f'D2BA_{corpus}', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "import pickle\n",
        "D2B_compound_map = {}\n",
        "with open('D2B_compound_map.pickle', 'rb') as f:\n",
        "  D2B_compound_map = pickle.load(f)\n",
        "\n",
        "with open('D2BA_noising_map.pickle', 'wb') as f:\n",
        "  pickle.dump(D2BA_noising_map, f)\n",
        "\n",
        "with open('D2BA_combined_map.pickle', 'wb') as f:\n",
        "  pickle.dump({**D2B_compound_map, **D2BA_noising_map}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "7wq4NWlFdb25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login\n",
        "!gcloud auth application-default set-quota-project resolute-parity-392608\n",
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOz8lNRseiWp",
        "outputId": "cf07ede7-529a-426d-a417-7d730905f847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=pn3jj5BdTojkiMLb0rEZFrWLXOSD1p&prompt=consent&access_type=offline&code_challenge=lxJSODtVjS4qAF4EzPHeZv7J9i-hld5sSNliI7eY3_I&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AfJohXkUDwBTRmWfhgszDOWBMt_iwnTbYGx7ErwsUgHynqk92JLdtjh1CxfT0zY0HWK5IQ\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"resolute-parity-392608\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=lXHvwJIi80IIYmdGqICiMwIJTFdZXj&prompt=consent&access_type=offline&code_challenge=WuFhb-cWrymEt75K_tlsCo0InEiDopwzPXHYpgdw_Lo&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AfJohXlkFwQN9Phc-YBmL2UdC5EeJsJs52_HMxz9VRU_TifjWP3hvVP8mddhkJLNWPZYHg\n",
            "\n",
            "You are now logged in as [luckyblock0@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "def romanize_text(src, contents):\n",
        "    # Define the data you want to send in the POST request\n",
        "    data = {\n",
        "        'source_language_code': src,\n",
        "        'contents': contents\n",
        "    }\n",
        "\n",
        "    # Define the file path where you want to save the JSON data\n",
        "    file_path = 'request.json'\n",
        "\n",
        "    # Write the dictionary to the JSON file\n",
        "    with open(file_path, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    command = \"\"\"\n",
        "    curl -X POST \\\n",
        "        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "        -H \"x-goog-user-project: resolute-parity-392608\" \\\n",
        "        -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "        -d @request.json \\\n",
        "        \"https://translation.googleapis.com/v3/projects/resolute-parity-392608/locations/us-central1:romanizeText\" \\\n",
        "        -o \"romanized.json\"\n",
        "    \"\"\"\n",
        "    subprocess.run(command, shell=True, text=True, stdout=subprocess.PIPE)\n",
        "\n",
        "    romanized_text_list = []\n",
        "    with open('romanized.json', 'r') as f:\n",
        "      romanized_texts = dict(json.load(f))['romanizations']\n",
        "\n",
        "      for roman_text in romanized_texts:\n",
        "        romanized_text_list.append(roman_text['romanizedText'])\n",
        "\n",
        "    return romanized_text_list\n",
        "\n",
        "# romanize_text('ja', ['お元気ですか', '私は元気です、あなたはどうですか'])\n",
        "\n",
        "def translate_text(target: str, source: str, text: str) -> dict:\n",
        "    \"\"\"Translates text into the target language.\n",
        "\n",
        "    Target must be an ISO 639-1 language code.\n",
        "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
        "    \"\"\"\n",
        "    from google.cloud import translate_v2 as translate\n",
        "\n",
        "    translate_client = translate.Client()\n",
        "\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode(\"utf-8\")\n",
        "\n",
        "    result = translate_client.translate(text, target_language=target, source_language=source)\n",
        "\n",
        "    non_roman = {\n",
        "      'ar': 'Arabic',\n",
        "      'am': 'Amharic',\n",
        "      'bn': 'Bengali',\n",
        "      'be': 'Belarusian',\n",
        "      'hi': 'Hindi',\n",
        "      'ja': 'Japanese',\n",
        "      'my': 'Myanmar',\n",
        "      'ru': 'Russian',\n",
        "      'sr': 'Serbian',\n",
        "      'uk': 'Ukrainian'\n",
        "    }\n",
        "\n",
        "    if target in non_roman.keys():\n",
        "      result = romanize_text(target, result['translatedText'])\n",
        "      return result\n",
        "    return result['translatedText']"
      ],
      "metadata": {
        "id": "1J_-PTaygYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "en_words = []\n",
        "en_dicts = {}\n",
        "\n",
        "with open('translate_map.pickle', 'rb') as f:\n",
        "  en_dicts = pickle.load(f)\n",
        "\n",
        "en_words = list(en_dicts.keys())"
      ],
      "metadata": {
        "id": "R3NssYNvge9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.alias import default_aliases\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "\n",
        "prob = 0.2 # 20% of the dictionary will be cognated\n",
        "flag_translate = [1] * len(en_words)\n",
        "\n",
        "i = 0\n",
        "translate_map = {}\n",
        "for word in en_words:\n",
        "  translate_map[word] = flag_translate[i]\n",
        "  i += 1\n",
        "\n",
        "C1_mapping = {}\n",
        "C2_mapping = {}\n",
        "D1_mapping = {}\n",
        "D2_mapping = {}\n",
        "\n",
        "languages = ['de', 'pt', 'af', 'gl']\n",
        "\n",
        "de_cache = {}\n",
        "pt_cache = {}\n",
        "af_cache = {}\n",
        "gl_cache = {}\n",
        "\n",
        "def transform_de_map(translate_map, tgt):\n",
        "  global de_cache\n",
        "  result_map = {}\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      if word in de_cache:\n",
        "        continue\n",
        "      else:\n",
        "        result_map[word] = translate_text(tgt, 'en', word)\n",
        "        de_cache[word] = result_map[word]\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "      de_cache[word] = result_map[word]\n",
        "  return result_map\n",
        "\n",
        "def transform_pt_map(translate_map, tgt):\n",
        "  global pt_cache\n",
        "  result_map = {}\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      if word in de_cache:\n",
        "        continue\n",
        "      else:\n",
        "        result_map[word] = translate_text(tgt, 'en', word)\n",
        "        pt_cache[word] = result_map[word]\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "      pt_cache[word] = result_map[word]\n",
        "  return result_map\n",
        "\n",
        "def transform_gl_map(translate_map, tgt):\n",
        "  global gl_cache\n",
        "  result_map = {}\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      if word in gl_cache:\n",
        "        continue\n",
        "      else:\n",
        "        result_map[word] = translate_text(tgt, 'en', word)\n",
        "        gl_cache[word] = result_map[word]\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "      gl_cache[word] = result_map[word]\n",
        "  return result_map"
      ],
      "metadata": {
        "id": "beVTJjHzgiKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "en_de_map = transform_de_map(translate_map, 'de')\n",
        "with open('en_de_map.pickle', 'wb') as f:\n",
        "    pickle.dump(en_de_map, f)\n",
        "files.download('en_de_map.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ik-1FfJNg7xx",
        "outputId": "3fab5f86-779d-47f0-953b-7f609d2c082e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10608/10608 [2:06:52<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_996eb1c3-dac4-4f03-b461-44ed46313c54\", \"en_de_map.pickle\", 221935)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "def transform_pt_map(translate_map, tgt):\n",
        "  global pt_cache\n",
        "  result_map = {}\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      if word in pt_cache:\n",
        "        continue\n",
        "      else:\n",
        "        result_map[word] = translate_text(tgt, 'en', word)\n",
        "        pt_cache[word] = result_map[word]\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "      pt_cache[word] = result_map[word]\n",
        "  return result_map\n",
        "\n",
        "en_pt_map = transform_pt_map(translate_map, 'pt')\n",
        "with open('en_pt_map.pickle', 'wb') as f:\n",
        "    pickle.dump(en_pt_map, f)\n",
        "files.download('en_pt_map.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xBjVmV88hHgA",
        "outputId": "48a977f6-d206-4556-82da-4da5cca60e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10608/10608 [2:07:08<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e358b67b-0d49-4901-8a72-688a6debc93c\", \"en_pt_map.pickle\", 222308)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "def transform_af_map(translate_map, tgt, default = {}):\n",
        "  i = 1\n",
        "  result_map = default\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      if word in af_cache:\n",
        "        continue\n",
        "      else:\n",
        "        result_map[word] = translate_text(tgt, 'en', word)\n",
        "        af_cache[word] = result_map[word]\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "      af_cache[word] = result_map[word]\n",
        "\n",
        "    if (i % 1000) == 0:\n",
        "      with open(f'en_af_map-{i}.pickle', 'wb') as f:\n",
        "        pickle.dump(result_map, f)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return result_map\n",
        "\n",
        "en_af_map = transform_af_map(translate_map, 'af')\n",
        "with open('en_af_map.pickle', 'wb') as f:\n",
        "    pickle.dump(en_af_map, f)\n",
        "files.download('en_af_map.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0DtssyMthII6",
        "outputId": "088113c0-b32f-4c52-e46e-1807e5c52f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10608/10608 [16:50<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c9022438-e790-4907-8775-86c514fd9719\", \"en_af_map.pickle\", 15169)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('en_af_map.pickle', 'wb') as f:\n",
        "    pickle.dump(af_cache, f)\n",
        "files.download('en_af_map.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UE3iNnt2Vy7T",
        "outputId": "a15ebff3-5ff3-4154-983b-b59238b7b2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fc177149-b9c0-4c97-975d-a371fa89248e\", \"en_af_map.pickle\", 215140)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(af_cache)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i-uB-0WapZ7",
        "outputId": "08582eb2-909a-480a-c1a7-52f2975308af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10608"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "def transform_gl_map(translate_map, tgt, default = {}):\n",
        "  i = 1\n",
        "  result_map = default\n",
        "  for word, flag in tqdm(translate_map.items()):\n",
        "    if flag:\n",
        "      if word in gl_cache:\n",
        "        continue\n",
        "      else:\n",
        "        result_map[word] = translate_text(tgt, 'en', word)\n",
        "        gl_cache[word] = result_map[word]\n",
        "    else:\n",
        "      result_map[word] = word\n",
        "      gl_cache[word] = result_map[word]\n",
        "\n",
        "    if (i % 1000) == 0:\n",
        "      with open(f'en_gl_map-{i}.pickle', 'wb') as f:\n",
        "        pickle.dump(result_map, f)\n",
        "      files.download(f'en_gl_map-{i}.pickle')\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return result_map\n",
        "\n",
        "en_gl_map = transform_gl_map(translate_map, 'gl')\n",
        "with open('en_gl_map.pickle', 'wb') as f:\n",
        "    pickle.dump(en_gl_map, f)\n",
        "files.download('en_gl_map.pickle')"
      ],
      "metadata": {
        "id": "D3VkRQ5_hItH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fKs0JGsnjEkC",
        "uaCM_-1rjIIn"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}