{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bxmjIpyDJxSQ",
        "w9v75LSeNwMQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!zip -r prompts_v2.zip experiment_prompts"
      ],
      "metadata": {
        "id": "lJW1MowUgaj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Experiments List:\n",
        "1. Bilingual Prompting, but randomize sequence of translation given in prompt.*\n",
        "2. Bilingual Prompting, but remove some of the most common/rare words translation, and see how much it hurts the model performance.\n",
        "3. Fewshot Prompting, but switch location of your data. Instead of fewshot first, dictionary entries first\n",
        "4. Fewshot Prompting, but only give 5 random examples\n",
        "5. Fewshot Prompting, but, change a couple of target word from fewshot examples to become a dictionary entries instead.\n",
        "6. Kitchen Sink: Gives full bilingual dictionary, then fewshot examples.\n",
        "\n",
        "*Currently, the sequence of translation is almost ideal. Ideal sequence of translation can only be achieved if we repeat word level translation sequences."
      ],
      "metadata": {
        "id": "914mvtjmlZGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://artificial-language-prerequisites.s3.amazonaws.com/Experiment_Data.zip"
      ],
      "metadata": {
        "id": "4fTd06PbtIRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "ZIP_FILE_PATH = \"Experiment_Data.zip\"\n",
        "TEMP_DIR = \"tempextract/\"\n",
        "EXTRACT_DIR = \"extract/\"\n",
        "\n",
        "import os\n",
        "os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "  zip_ref.extractall(TEMP_DIR)\n",
        "\n",
        "import shutil\n",
        "for root, dirs, files in os.walk(TEMP_DIR):\n",
        "  for dir in dirs:\n",
        "    os.makedirs(os.path.join(EXTRACT_DIR, dir), exist_ok=True)\n",
        "  for file_ in files:\n",
        "    group = file_.split('_')[0]\n",
        "    target_dir = os.path.join(EXTRACT_DIR, group)\n",
        "\n",
        "    source_file = os.path.join(root, file_)\n",
        "    if os.path.exists(target_dir) and os.path.isdir(target_dir):\n",
        "      shutil.copy2(source_file, target_dir)\n",
        "    else:\n",
        "      shutil.copy2(source_file, EXTRACT_DIR)\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree(TEMP_DIR)"
      ],
      "metadata": {
        "id": "MBR1_BSYtY0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper func\n",
        "def load_map(f_name):\n",
        "  import pickle\n",
        "  with open(f_name, 'rb') as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "def load_file(f_name):\n",
        "  with open(f_name, 'r') as f:\n",
        "    return f.readlines()\n",
        "## End of helper func"
      ],
      "metadata": {
        "id": "Q1ZAUcJ7ttt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts v1"
      ],
      "metadata": {
        "id": "vwOENggV0dI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Bilingual Prompting with randomized sequence"
      ],
      "metadata": {
        "id": "5kTczb8BnWWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB"
      ],
      "metadata": {
        "id": "5LGm83tB2mLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "UEcH9Nby2lhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EI_sBWK29Uw"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def AB_bilingual_prompting(input_sentence, label_sentence, noising_map, compound_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_bilingual_prompting\n",
        "  # Uses the tokenize function above\n",
        "  ### End of Explanation\n",
        "  LABEL_TOKENS = tokenize(label_sentence)\n",
        "  R_NOISING_MAP = {}\n",
        "  for k,v in noising_map.items():\n",
        "    R_NOISING_MAP[v] = k\n",
        "\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  NOISED_TOKENS = []\n",
        "  COMPOUNDED_TOKENS = []\n",
        "\n",
        "  for token in LABEL_TOKENS:\n",
        "    if token in R_NOISING_MAP.keys():\n",
        "      NOISED_TOKENS.append(token)\n",
        "    if token in R_COMPOUND_MAP.keys():\n",
        "      COMPOUNDED_TOKENS.append(token)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += \"The following is a list of word translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "\n",
        "  # Augmented with random ordering\n",
        "  random.shuffle(COMPOUNDED_TOKENS)\n",
        "  random.shuffle(NOISED_TOKENS)\n",
        "\n",
        "  for token in COMPOUNDED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_COMPOUND_MAP[token][0]} {R_COMPOUND_MAP[token][1]}\" means \"{token}\"\\n'\n",
        "\n",
        "  for token in NOISED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_NOISING_MAP[token]}\" means \"{token}\"\\n'\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_bilingual_prompting(input_file, label_files, noising_map, compound_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform bilingual prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'bilingual_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'bilingual_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file in label_files:\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_bilingual_prompting(input_sentence, label_sentences[idx], noising_map, compound_map, EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "149gjUr62txX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_bilingual_prompting(INPUT_FILE,\n",
        "                                  LABEL_FILES,\n",
        "                                  NOISING_MAP,\n",
        "                                  COMPOUND_MAP,\n",
        "                                  LOG_DIR,\n",
        "                                  RESULT_DIR,\n",
        "                                  log_fname = \"randomized_order\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "id": "Xi9-ebzW3Bz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6dd43e-db4e-4408-82f5-13971fa21f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/bilingual_prompting\n",
            "res_dir = experiment_results/AB/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:03<00:00, 334.46it/s]\n",
            "100%|██████████| 1012/1012 [00:03<00:00, 322.03it/s]\n",
            "100%|██████████| 1012/1012 [00:03<00:00, 260.90it/s]\n",
            "100%|██████████| 1012/1012 [00:02<00:00, 348.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA"
      ],
      "metadata": {
        "id": "BqAJqkVQ35lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def CDBA_bilingual_prompting(input_sentence, label_sentence, translate_map, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_bilingual_prompting\n",
        "  # Uses the tokenize function above (tbh, they are all the same and unchanged)\n",
        "  ### End of Explanation\n",
        "\n",
        "  # 0. Prepare prompt\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += \"The following is a list of word translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "\n",
        "  INPUT_TOKENS = tokenize(input_sentence)\n",
        "  random.shuffle(INPUT_TOKENS)\n",
        "  # 1. Original --> C/Dx\n",
        "  for token in INPUT_TOKENS:\n",
        "    if token in translate_map:\n",
        "      if token not in ALREADY_TRANSLATED:\n",
        "        ALREADY_TRANSLATED.append(token)\n",
        "        ALREADY_TRANSLATED.append(TRANSLATE_MAP[token])\n",
        "\n",
        "        if TRANSLATE_MAP[token] in NOISING_MAP:\n",
        "          prompt += f'\"{token}\" means \"{NOISING_MAP[TRANSLATE_MAP[token]]}\"\\n'\n",
        "          ALREADY_TRANSLATED.append(NOISING_MAP[TRANSLATE_MAP[token]])\n",
        "        else:\n",
        "          prompt += f'\"{token}\" means \"{TRANSLATE_MAP[token]}\"\\n'\n",
        "\n",
        "\n",
        "  # 2. C/Dx --> C/DxB --> C/DxBA\n",
        "  ## Note, Even though the A + B experiment is named AB,\n",
        "  ## It actually perform compounding first, THEN noising\n",
        "  ## Just like this one.\n",
        "  LABEL_TOKENS = tokenize(label_sentence)\n",
        "\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  R_NOISING_MAP = {}\n",
        "  for k,v in noising_map.items():\n",
        "    R_NOISING_MAP[v] = k\n",
        "\n",
        "  COMPOUNDED_TOKENS = []\n",
        "  NOISED_TOKENS = []\n",
        "\n",
        "  for token in LABEL_TOKENS:\n",
        "    if token in R_NOISING_MAP.keys():\n",
        "      NOISED_TOKENS.append(token)\n",
        "      if token in R_COMPOUND_MAP.keys():\n",
        "        COMPOUNDED_TOKENS.append(token)\n",
        "\n",
        "  random.shuffle(COMPOUNDED_TOKENS)\n",
        "  random.shuffle(NOISED_TOKENS)\n",
        "  for token in COMPOUNDED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_COMPOUND_MAP[token][0]} {R_COMPOUND_MAP[token][1]}\" means \"{token}\"\\n'\n",
        "\n",
        "  for token in NOISED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_NOISING_MAP[token]}\" means \"{token}\"\\n'\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "ByN7Il0z4GN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_bilingual_prompting(input_file, label_files, translate_map, compound_map, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform bilingual prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'CDBA'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'bilingual_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'bilingual_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file in label_files:\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_bilingual_prompting(input_sentence, label_sentences[idx], translate_map, compound_map, noising_map, EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "VS45GmQF4drU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D1"
      ],
      "metadata": {
        "id": "t3K9Z9oR4mmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "fpYm9AOU6fOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## D1BA - bilingual prompting\n",
        "EXPERIMENT_CDBA_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                    label_files = LABEL_FILES,\n",
        "                                    translate_map = TRANSLATE_MAP,\n",
        "                                    compound_map = COMPOUND_MAP,\n",
        "                                    noising_map = NOISING_MAP,\n",
        "                                    log_dir = LOG_DIR,\n",
        "                                    result_dir = RESULT_DIR,\n",
        "                                    log_fname = \"randomized_order\",\n",
        "                                    result_fname = \"\")"
      ],
      "metadata": {
        "id": "5kczjYxJ6gqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa5f7e6-e198-4964-aaa9-25024cf6e512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/bilingual_prompting\n",
            "res_dir = experiment_results/D1BA/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:04<00:00, 232.14it/s]\n",
            "100%|██████████| 1012/1012 [00:05<00:00, 176.47it/s]\n",
            "100%|██████████| 1012/1012 [00:04<00:00, 238.54it/s]\n",
            "100%|██████████| 1012/1012 [00:04<00:00, 221.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bilingual Mapping with common/rare words removed"
      ],
      "metadata": {
        "id": "2siiCQAR63yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fewshot Prompting, but dictionary entries first, then fewshot examples"
      ],
      "metadata": {
        "id": "zRzgP7UG7MJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB"
      ],
      "metadata": {
        "id": "bbWwY6j9CVR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def AB_fewshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # AB.1 and AB.2 had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes = []\n",
        "  unfound_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      few_shot_indexes.append(get_index(input_token, fewshot_input_sentences))\n",
        "    elif isinstance(idx, str):\n",
        "      unfound_word.append(idx)\n",
        "\n",
        "  few_shot_indexes = list(set(few_shot_indexes))\n",
        "  unfound_word = list(set(unfound_word))\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += f\"The following is a list of sentence translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  for w in unfound_word:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {w}\\n\"\n",
        "    prompt += f\"Exurbanta: {noising_map.get(w, w)}\\n\"\n",
        "\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "iJwDibl1CjQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_fewshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_fewshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "_ndQEJ1pCk1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "Btg9ubqUDVh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_fewshot_prompting(\n",
        "    INPUT_FILE,\n",
        "    LABEL_FILES,\n",
        "    FEWSHOT_INPUT_FILE,\n",
        "    FEWSHOT_LABEL_FILES,\n",
        "    NOISING_MAP,\n",
        "    LOG_DIR,\n",
        "    RESULT_DIR,\n",
        "    log_fname = \"entry_then_fewshow\",\n",
        "    result_fname = \"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwdb4Q-QCmh9",
        "outputId": "42bc80a3-3ade-4ca9-808e-63925c8ec9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/fewshot_prompting\n",
            "res_dir = experiment_results/AB/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:04<00:00, 232.09it/s]\n",
            "100%|██████████| 1012/1012 [00:03<00:00, 281.15it/s]\n",
            "100%|██████████| 1012/1012 [00:03<00:00, 284.60it/s]\n",
            "100%|██████████| 1012/1012 [00:04<00:00, 214.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA"
      ],
      "metadata": {
        "id": "WcbPJQw_CX5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def CDBA_fewshot_prompting(input_sentence, label_sentence, fewshot_input_file, fewshot_label_file, translation_map, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # Other prompting functions had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes = []\n",
        "  unfound_en_word = []\n",
        "  unfound_exurbanta_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      if idx not in few_shot_indexes:\n",
        "        few_shot_indexes.append(idx)\n",
        "    elif isinstance(idx, str):\n",
        "      if idx not in unfound_en_word:\n",
        "        unfound_en_word.append(idx)\n",
        "\n",
        "  label_tokens = tokenize(label_sentence)\n",
        "  for label_token in label_tokens:\n",
        "    idx = get_index(label_token, fewshot_label_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      if idx not in few_shot_indexes:\n",
        "        few_shot_indexes.append(idx)\n",
        "    elif isinstance(idx, str):\n",
        "      if idx not in unfound_exurbanta_word:\n",
        "        unfound_exurbanta_word.append(idx)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += f\"The following is a list of sentence translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "  # Handles unhandled Noising\n",
        "  for w in unfound_en_word:\n",
        "    if w not in ALREADY_TRANSLATED:\n",
        "      prompt += f\"\\n\"\n",
        "      prompt += f\"English: {w}\\n\"\n",
        "      if w in noising_map:\n",
        "        ALREADY_TRANSLATED.append(noising_map[w])\n",
        "        prompt += f\"Exurbanta: {noising_map[w]}\\n\"\n",
        "      else:\n",
        "        prompt += f\"Exurbanta: {w}\\n\"\n",
        "      ALREADY_TRANSLATED.append(w)\n",
        "\n",
        "  # Handles unhandled Compounding\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  for w in unfound_exurbanta_word:\n",
        "    if w not in ALREADY_TRANSLATED:\n",
        "      prompt += f\"\\n\"\n",
        "      if w in R_COMPOUND_MAP:\n",
        "        prompt += f\"English: {R_COMPOUND_MAP[w][0]} {R_COMPOUND_MAP[w][1]}\\n\"\n",
        "        prompt += f\"Exurbanta: {w}\\n\"\n",
        "        ALREADY_TRANSLATED.append(w)\n",
        "\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "rXKHAHLED6eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_fewshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translate_map, compound_map, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_fewshot_prompting(input_sentence,\n",
        "                                      label_sentences[idx],\n",
        "                                      fewshot_input_file,\n",
        "                                      fewshot_label_file,\n",
        "                                      translate_map,\n",
        "                                      compound_map,\n",
        "                                      noising_map,\n",
        "                                      EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = 'test'\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "QQjKnH9QD82y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D1"
      ],
      "metadata": {
        "id": "KZxVn_3ICaMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "tleBSJ4FEG-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_fewshot_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  fewshot_input_file = FEWSHOT_INPUT_FILE,\n",
        "                                  fewshot_label_files = FEWSHOT_LABEL_FILES,\n",
        "                                  translate_map = TRANSLATE_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"entry_then_fewshow\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQAc-aOeEKRn",
        "outputId": "8c3d5fc5-2257-48da-ea4a-724d66c30829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/fewshot_prompting\n",
            "res_dir = experiment_results/D1BA/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:07<00:00, 144.36it/s]\n",
            "100%|██████████| 1012/1012 [00:08<00:00, 125.51it/s]\n",
            "100%|██████████| 1012/1012 [00:07<00:00, 141.64it/s]\n",
            "100%|██████████| 1012/1012 [00:08<00:00, 125.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fewshot Prompting, but only give 5 random examples"
      ],
      "metadata": {
        "id": "htDZpY8DJOHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB"
      ],
      "metadata": {
        "id": "bxmjIpyDJxSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def AB_fiveshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # AB.1 and AB.2 had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "\n",
        "  few_shot_indexes = random.sample(range(1, 998), 5)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += f\"The following is a list of sentence translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "Yz_GhiRtKCCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_fiveshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_fiveshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "aGJZS_AtKFNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "5aBtBrTJKKeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_fiveshot_prompting(\n",
        "    INPUT_FILE,\n",
        "    LABEL_FILES,\n",
        "    FEWSHOT_INPUT_FILE,\n",
        "    FEWSHOT_LABEL_FILES,\n",
        "    NOISING_MAP,\n",
        "    LOG_DIR,\n",
        "    RESULT_DIR,\n",
        "    log_fname = \"fiveshot\",\n",
        "    result_fname = \"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c842757a-745a-4599-b23e-9be133e9f2a0",
        "id": "oDM04PuTKKeO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/fewshot_prompting\n",
            "res_dir = experiment_results/AB/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:01<00:00, 626.42it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 602.44it/s]\n",
            "100%|██████████| 1012/1012 [00:02<00:00, 432.94it/s]\n",
            "100%|██████████| 1012/1012 [00:02<00:00, 489.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA"
      ],
      "metadata": {
        "id": "w9v75LSeNwMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def CDBA_fiveshot_prompting(input_sentence, label_sentence, fewshot_input_file, fewshot_label_file, translation_map, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # Other prompting functions had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  few_shot_indexes = random.sample(range(1, 998), 5)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += f\"The following is a list of sentence translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "IbcMCwkDN0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_fiveshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translate_map, compound_map, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_fiveshot_prompting(input_sentence,\n",
        "                                      label_sentences[idx],\n",
        "                                      fewshot_input_file,\n",
        "                                      fewshot_label_file,\n",
        "                                      translate_map,\n",
        "                                      compound_map,\n",
        "                                      noising_map,\n",
        "                                      EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = 'test'\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "331-yA-kORgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D1"
      ],
      "metadata": {
        "id": "a6IFsvFOJ5wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "H70PdbOvOdEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_fiveshot_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  fewshot_input_file = FEWSHOT_INPUT_FILE,\n",
        "                                  fewshot_label_files = FEWSHOT_LABEL_FILES,\n",
        "                                  translate_map = TRANSLATE_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"fiveshot\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au0Yec5oOfsG",
        "outputId": "fe25a0d9-9ad4-4103-a057-d3d3c640ed0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/fewshot_prompting\n",
            "res_dir = experiment_results/D1BA/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:01<00:00, 649.82it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 647.43it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 644.76it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 649.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Fewshot Prompting, but, change a couple of target word from fewshot examples to become a dictionary entries instead."
      ],
      "metadata": {
        "id": "sQbokd_yJPnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB"
      ],
      "metadata": {
        "id": "vr0WvLoqJyrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return (in_token, idx)\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def AB_mixed_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # AB.1 and AB.2 had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes = []\n",
        "  unfound_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, tuple):\n",
        "      few_shot_indexes.append(get_index(input_token, fewshot_input_sentences))\n",
        "    elif isinstance(idx, str):\n",
        "      unfound_word.append(idx)\n",
        "\n",
        "  few_shot_indexes = list(set(few_shot_indexes))\n",
        "  unfound_word = list(set(unfound_word))\n",
        "\n",
        "  random.shuffle(few_shot_indexes)\n",
        "\n",
        "  half_len = len(few_shot_indexes)//2\n",
        "  temp_shots = few_shot_indexes[:half_len]\n",
        "  few_shot_indexes = few_shot_indexes[half_len:]\n",
        "\n",
        "  for token, idx in temp_shots:\n",
        "    unfound_word.append(token)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += f\"The following is a list of sentence translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  for w in unfound_word:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {w}\\n\"\n",
        "    prompt += f\"Exurbanta: {noising_map.get(w, w)}\\n\"\n",
        "\n",
        "  for token, idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "ZZtKwrtyUqXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_mixed_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_mixed_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "6e3UlpKeZeOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "htFt3hkdZofy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_mixed_prompting(\n",
        "    INPUT_FILE,\n",
        "    LABEL_FILES,\n",
        "    FEWSHOT_INPUT_FILE,\n",
        "    FEWSHOT_LABEL_FILES,\n",
        "    NOISING_MAP,\n",
        "    LOG_DIR,\n",
        "    RESULT_DIR,\n",
        "    log_fname = \"mixed\",\n",
        "    result_fname = \"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e09c11-3884-4386-ef96-77fdedc929d7",
        "id": "FIc2tYN5Zof1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/fewshot_prompting\n",
            "res_dir = experiment_results/AB/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:04<00:00, 233.47it/s]\n",
            "100%|██████████| 1012/1012 [00:04<00:00, 249.05it/s]\n",
            "100%|██████████| 1012/1012 [00:03<00:00, 279.19it/s]\n",
            "100%|██████████| 1012/1012 [00:03<00:00, 271.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA"
      ],
      "metadata": {
        "id": "kCaER13OJ2zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return (in_token, idx)\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def CDBA_mixed_prompting(input_sentence, label_sentence, fewshot_input_file, fewshot_label_file, translation_map, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # Other prompting functions had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes_en = []\n",
        "  few_shot_indexes_exurbanta = []\n",
        "  unfound_en_word = []\n",
        "  unfound_exurbanta_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, tuple):\n",
        "      if idx not in few_shot_indexes_en:\n",
        "        few_shot_indexes_en.append(idx)\n",
        "    elif isinstance(idx, str):\n",
        "      if idx not in unfound_en_word:\n",
        "        unfound_en_word.append(idx)\n",
        "\n",
        "  label_tokens = tokenize(label_sentence)\n",
        "  for label_token in label_tokens:\n",
        "    idx = get_index(label_token, fewshot_label_sentences)\n",
        "    if isinstance(idx, tuple):\n",
        "      if idx not in few_shot_indexes_exurbanta:\n",
        "        few_shot_indexes_exurbanta.append(idx)\n",
        "    elif isinstance(idx, str):\n",
        "      if idx not in unfound_exurbanta_word:\n",
        "        unfound_exurbanta_word.append(idx)\n",
        "\n",
        "  few_shot_indexes = []\n",
        "  random.shuffle(few_shot_indexes_en)\n",
        "\n",
        "  half_len = len(few_shot_indexes_en)//2\n",
        "  temp_shots = few_shot_indexes_en[:half_len]\n",
        "  few_shot_indexes = few_shot_indexes[half_len:]\n",
        "\n",
        "  for token, idx in temp_shots:\n",
        "    unfound_en_word.append(token)\n",
        "\n",
        "  random.shuffle(few_shot_indexes_exurbanta)\n",
        "\n",
        "  half_len = len(few_shot_indexes_exurbanta)//2\n",
        "  temp_shots = few_shot_indexes_exurbanta[:half_len]\n",
        "  few_shot_indexes += few_shot_indexes_exurbanta[half_len:]\n",
        "\n",
        "  for token, idx in temp_shots:\n",
        "    unfound_exurbanta_word.append(token)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += f\"The following is a list of sentence translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "  # Handles unhandled Noising\n",
        "  for w in unfound_en_word:\n",
        "    if w not in ALREADY_TRANSLATED:\n",
        "      prompt += f\"\\n\"\n",
        "      prompt += f\"English: {w}\\n\"\n",
        "      if w in noising_map:\n",
        "        ALREADY_TRANSLATED.append(noising_map[w])\n",
        "        prompt += f\"Exurbanta: {noising_map[w]}\\n\"\n",
        "      else:\n",
        "        prompt += f\"Exurbanta: {w}\\n\"\n",
        "      ALREADY_TRANSLATED.append(w)\n",
        "\n",
        "  # Handles unhandled Compounding\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  for w in unfound_exurbanta_word:\n",
        "    if w not in ALREADY_TRANSLATED:\n",
        "      if w in R_COMPOUND_MAP:\n",
        "        prompt += f\"\\n\"\n",
        "        prompt += f\"English: {R_COMPOUND_MAP[w][0]} {R_COMPOUND_MAP[w][1]}\\n\"\n",
        "        prompt += f\"Exurbanta: {w}\\n\"\n",
        "        ALREADY_TRANSLATED.append(w)\n",
        "\n",
        "  for token, idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "jtB4JQNyanzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_mixed_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translate_map, compound_map, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_mixed_prompting(input_sentence,\n",
        "                                      label_sentences[idx],\n",
        "                                      fewshot_input_file,\n",
        "                                      fewshot_label_file,\n",
        "                                      translate_map,\n",
        "                                      compound_map,\n",
        "                                      noising_map,\n",
        "                                      EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = 'test'\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "H7-No0fTarhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D1"
      ],
      "metadata": {
        "id": "V6LwWEtYJ6ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "K1KH1QxBBJp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_mixed_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  fewshot_input_file = FEWSHOT_INPUT_FILE,\n",
        "                                  fewshot_label_files = FEWSHOT_LABEL_FILES,\n",
        "                                  translate_map = TRANSLATE_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"mixed\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s8RkvqbBMLY",
        "outputId": "8d271749-2691-4f00-c1a6-a4a401a95557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/fewshot_prompting\n",
            "res_dir = experiment_results/D1BA/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:08<00:00, 120.92it/s]\n",
            "100%|██████████| 1012/1012 [00:08<00:00, 120.22it/s]\n",
            "100%|██████████| 1012/1012 [00:07<00:00, 138.90it/s]\n",
            "100%|██████████| 1012/1012 [00:08<00:00, 116.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Kitchen Sink: Gives full bilingual dictionary, then fewshot examples."
      ],
      "metadata": {
        "id": "8C4GODJoJWQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB"
      ],
      "metadata": {
        "id": "xmMw-CNcJzyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def AB_kitchensink_prompting(input_sentence, label_sentence, fewshot_input_file, fewshot_label_file, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_bilingual_prompting\n",
        "  # Uses the tokenize function above\n",
        "  ### End of Explanation\n",
        "  LABEL_TOKENS = tokenize(label_sentence)\n",
        "  R_NOISING_MAP = {}\n",
        "  for k,v in noising_map.items():\n",
        "    R_NOISING_MAP[v] = k\n",
        "\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  NOISED_TOKENS = []\n",
        "  COMPOUNDED_TOKENS = []\n",
        "\n",
        "  for token in LABEL_TOKENS:\n",
        "    if token in R_NOISING_MAP.keys():\n",
        "      NOISED_TOKENS.append(token)\n",
        "    if token in R_COMPOUND_MAP.keys():\n",
        "      COMPOUNDED_TOKENS.append(token)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += \"The following is a list of word translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "\n",
        "  for token in COMPOUNDED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_COMPOUND_MAP[token][0]} {R_COMPOUND_MAP[token][1]}\" means \"{token}\"\\n'\n",
        "\n",
        "  for token in NOISED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_NOISING_MAP[token]}\" means \"{token}\"\\n'\n",
        "\n",
        "  # ========= FEW SHOT ===========\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes = []\n",
        "  unfound_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      few_shot_indexes.append(get_index(input_token, fewshot_input_sentences))\n",
        "    elif isinstance(idx, str):\n",
        "      unfound_word.append(idx)\n",
        "\n",
        "  few_shot_indexes = list(set(few_shot_indexes))\n",
        "  unfound_word = list(set(unfound_word))\n",
        "\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "ujnu8Ki2EFXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_kitchensink_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, noising_map, compound_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_kitchensink_prompting(input_sentence = input_sentence,\n",
        "                                        label_sentence = label_sentences[idx],\n",
        "                                        fewshot_input_file = fewshot_input_file,\n",
        "                                        fewshot_label_file = fewshot_label_file,\n",
        "                                        compound_map = compound_map,\n",
        "                                        noising_map = noising_map,\n",
        "                                        word_order = EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "zIsU_L1CD3Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "3vaP2Uv4TEud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_kitchensink_prompting(\n",
        "    INPUT_FILE,\n",
        "    LABEL_FILES,\n",
        "    FEWSHOT_INPUT_FILE,\n",
        "    FEWSHOT_LABEL_FILES,\n",
        "    NOISING_MAP,\n",
        "    COMPOUND_MAP,\n",
        "    LOG_DIR,\n",
        "    RESULT_DIR,\n",
        "    log_fname = \"kitchensink\",\n",
        "    result_fname = \"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYKoWpI4TNma",
        "outputId": "c69ff055-d5d5-4f4d-dbb2-57c8ef368c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/fewshot_prompting\n",
            "res_dir = experiment_results/AB/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:08<00:00, 117.84it/s]\n",
            "100%|██████████| 1012/1012 [00:07<00:00, 140.81it/s]\n",
            "100%|██████████| 1012/1012 [00:08<00:00, 121.52it/s]\n",
            "100%|██████████| 1012/1012 [00:08<00:00, 123.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA"
      ],
      "metadata": {
        "id": "1nMySUt0J3bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def CDBA_kitchensink_prompting(input_sentence, label_sentence, fewshot_input_file, fewshot_label_file, translate_map, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_bilingual_prompting\n",
        "  # Uses the tokenize function above (tbh, they are all the same and unchanged)\n",
        "  ### End of Explanation\n",
        "\n",
        "  # 0. Prepare prompt\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += \"The following is a list of word translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "\n",
        "  INPUT_TOKENS = tokenize(input_sentence)\n",
        "  # 1. Original --> C/Dx\n",
        "  for token in INPUT_TOKENS:\n",
        "    if token in translate_map:\n",
        "      if token not in ALREADY_TRANSLATED:\n",
        "        ALREADY_TRANSLATED.append(token)\n",
        "        ALREADY_TRANSLATED.append(TRANSLATE_MAP[token])\n",
        "\n",
        "        if TRANSLATE_MAP[token] in NOISING_MAP:\n",
        "          prompt += f'\"{token}\" means \"{NOISING_MAP[TRANSLATE_MAP[token]]}\"\\n'\n",
        "          ALREADY_TRANSLATED.append(NOISING_MAP[TRANSLATE_MAP[token]])\n",
        "        else:\n",
        "          prompt += f'\"{token}\" means \"{TRANSLATE_MAP[token]}\"\\n'\n",
        "\n",
        "\n",
        "  # 2. C/Dx --> C/DxB --> C/DxBA\n",
        "  ## Note, Even though the A + B experiment is named AB,\n",
        "  ## It actually perform compounding first, THEN noising\n",
        "  ## Just like this one.\n",
        "  LABEL_TOKENS = tokenize(label_sentence)\n",
        "\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  R_NOISING_MAP = {}\n",
        "  for k,v in noising_map.items():\n",
        "    R_NOISING_MAP[v] = k\n",
        "\n",
        "  COMPOUNDED_TOKENS = []\n",
        "  NOISED_TOKENS = []\n",
        "\n",
        "  for token in LABEL_TOKENS:\n",
        "    if token in R_NOISING_MAP.keys():\n",
        "      NOISED_TOKENS.append(token)\n",
        "      if token in R_COMPOUND_MAP.keys():\n",
        "        COMPOUNDED_TOKENS.append(token)\n",
        "\n",
        "  for token in COMPOUNDED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_COMPOUND_MAP[token][0]} {R_COMPOUND_MAP[token][1]}\" means \"{token}\"\\n'\n",
        "\n",
        "  for token in NOISED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_NOISING_MAP[token]}\" means \"{token}\"\\n'\n",
        "\n",
        "  # ====== Fewshot ======\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes = []\n",
        "  unfound_en_word = []\n",
        "  unfound_exurbanta_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      if idx not in few_shot_indexes:\n",
        "        few_shot_indexes.append(idx)\n",
        "    elif isinstance(idx, str):\n",
        "      if idx not in unfound_en_word:\n",
        "        unfound_en_word.append(idx)\n",
        "\n",
        "  label_tokens = tokenize(label_sentence)\n",
        "  for label_token in label_tokens:\n",
        "    idx = get_index(label_token, fewshot_label_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      if idx not in few_shot_indexes:\n",
        "        few_shot_indexes.append(idx)\n",
        "    elif isinstance(idx, str):\n",
        "      if idx not in unfound_exurbanta_word:\n",
        "        unfound_exurbanta_word.append(idx)\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "  # Handles unhandled Noising\n",
        "  # for w in unfound_en_word:\n",
        "  #   if w not in ALREADY_TRANSLATED:\n",
        "  #     prompt += f\"\\n\"\n",
        "  #     prompt += f\"English: {w}\\n\"\n",
        "  #     if w in noising_map:\n",
        "  #       ALREADY_TRANSLATED.append(noising_map[w])\n",
        "  #       prompt += f\"Exurbanta: {noising_map[w]}\\n\"\n",
        "  #     else:\n",
        "  #       prompt += f\"Exurbanta: {w}\\n\"\n",
        "  #     ALREADY_TRANSLATED.append(w)\n",
        "\n",
        "  # Handles unhandled Compounding\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"\\n\"\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"Exurbanta: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "4r8p5g7SZQPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_kitchensink_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translate_map, compound_map, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_kitchensink_prompting(input_sentence = input_sentence,\n",
        "                                          label_sentence = label_sentences[idx],\n",
        "                                          fewshot_input_file = fewshot_input_file,\n",
        "                                          fewshot_label_file = fewshot_label_file,\n",
        "                                          translate_map = translate_map,\n",
        "                                          compound_map = compound_map,\n",
        "                                          noising_map = noising_map,\n",
        "                                          word_order = EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = 'test'\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "LJxiZfNPYo3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D1"
      ],
      "metadata": {
        "id": "LQJy3IvjJ8Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "GyDNoUGkbvth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_kitchensink_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  fewshot_input_file = FEWSHOT_INPUT_FILE,\n",
        "                                  fewshot_label_files = FEWSHOT_LABEL_FILES,\n",
        "                                  translate_map = TRANSLATE_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"kitchensink\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdpAgSrYbzF3",
        "outputId": "ce47efb9-0c69-4513-89d1-ed993ba47a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/fewshot_prompting\n",
            "res_dir = experiment_results/D1BA/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:14<00:00, 69.02it/s]\n",
            "100%|██████████| 1012/1012 [00:12<00:00, 77.95it/s]\n",
            "100%|██████████| 1012/1012 [00:13<00:00, 77.17it/s]\n",
            "100%|██████████| 1012/1012 [00:14<00:00, 70.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfHccKobcrT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts v2"
      ],
      "metadata": {
        "id": "RZMq5DYO0fpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('experiment_prompts')\n",
        "shutil.rmtree('experiment_results')"
      ],
      "metadata": {
        "id": "Qws5X7xBkQl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Fewshot Prompting, but only give 5 random examples (Follows dito's pattern)"
      ],
      "metadata": {
        "id": "NrhpG_1y0lD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB"
      ],
      "metadata": {
        "id": "mZTLWNdm0vjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def AB_fiveshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # AB.1 and AB.2 had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "\n",
        "  few_shot_indexes = random.sample(range(1, 998), 5)\n",
        "\n",
        "  prompt = \"This is an English to Exurbanta translation, please provide the Exurbanta translation for these sentences:\\n\"\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1].strip()} Exurbanta: {fewshot_label_sentences[idx-1][1].strip()}\\n\"\n",
        "  prompt += \"Please provide the translation for the following sentence.\\n\"\n",
        "  prompt += \"Do not provide any explanations or text apart from the translation.\\n\"\n",
        "  prompt += f\"English: {input_sentence.strip()}\\n\"\n",
        "  prompt += \"Exurbanta: \"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "FJFE5ax70vja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_fiveshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_fiveshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, noising_map, EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "uveltDWm0vjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "kMlh_0Ab0vjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_fiveshot_prompting(\n",
        "    INPUT_FILE,\n",
        "    LABEL_FILES,\n",
        "    FEWSHOT_INPUT_FILE,\n",
        "    FEWSHOT_LABEL_FILES,\n",
        "    NOISING_MAP,\n",
        "    LOG_DIR,\n",
        "    RESULT_DIR,\n",
        "    log_fname = \"template_fiveshot\",\n",
        "    result_fname = \"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3b04b1-6f42-4fa0-a57b-c300c5dfe833",
        "id": "ua-k1xu10vjd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/fewshot_prompting\n",
            "res_dir = experiment_results/AB/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:01<00:00, 555.31it/s]\n",
            "100%|██████████| 1012/1012 [00:02<00:00, 352.07it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 511.08it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 565.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA"
      ],
      "metadata": {
        "id": "XsFucEjN0vjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def CDBA_fiveshot_prompting(input_sentence, label_sentence, fewshot_input_file, fewshot_label_file, translation_map, compound_map, noising_map, word_order):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_fewshot_prompting\n",
        "  # Uses the tokenize function above\n",
        "  # This functiosn only require the input_sentence\n",
        "  # Other prompting functions had to use label_sentence because of the existance of compounding words\n",
        "  ### End of Explanation\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  few_shot_indexes = random.sample(range(1, 998), 5)\n",
        "\n",
        "  prompt = \"This is an English to Exurbanta translation, please provide the Exurbanta translation for these sentences:\\n\"\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1].strip()} Exurbanta: {fewshot_label_sentences[idx-1][1].strip()}\\n\"\n",
        "  prompt += \"Please provide the translation for the following sentence.\\n\"\n",
        "  prompt += \"Do not provide any explanations or text apart from the translation.\\n\"\n",
        "  prompt += f\"English: {input_sentence.strip()}\\n\"\n",
        "  prompt += \"Exurbanta: \"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "c0N_cqHX0vjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_fiveshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translate_map, compound_map, noising_map, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_fiveshot_prompting(input_sentence,\n",
        "                                      label_sentences[idx],\n",
        "                                      fewshot_input_file,\n",
        "                                      fewshot_label_file,\n",
        "                                      translate_map,\n",
        "                                      compound_map,\n",
        "                                      noising_map,\n",
        "                                      EXPERIMENT_order)\n",
        "      dialog = [\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = 'test'\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "32Ujbsmm0vjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D1"
      ],
      "metadata": {
        "id": "TZWI4chA0vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "ZKtfca800vji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_fiveshot_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  fewshot_input_file = FEWSHOT_INPUT_FILE,\n",
        "                                  fewshot_label_files = FEWSHOT_LABEL_FILES,\n",
        "                                  translate_map = TRANSLATE_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"template_fiveshot-test1\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0169b505-72c8-4cdb-decf-ef8fb085aef1",
        "id": "0Kvy3-J00vjj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/fewshot_prompting\n",
            "res_dir = experiment_results/D1BA/fewshot_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:01<00:00, 582.91it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 581.99it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 595.37it/s]\n",
            "100%|██████████| 1012/1012 [00:01<00:00, 511.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84jCgp5Y0kUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bilingual Mapping with Word Type 50% Masked\n",
        "--> ONLY RUN THIS FOR SVO"
      ],
      "metadata": {
        "id": "pp1ycE-A8ACW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT\n",
        "\n",
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_vso')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_sov'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vos'),\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_vso')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "HJcTuQeR8Cah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('universal_tagset')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# import nltk\n",
        "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "# from nltk.tag import pos_tag\n",
        "\n",
        "# sentences = []\n",
        "# with open('extract/flores_english_svo', 'r', encoding=\"utf-8\") as f:\n",
        "#   for line in f:\n",
        "#     sentences.append(line)\n",
        "\n",
        "# # Tokenize each sentence into words and tag with POS\n",
        "# word_counter = {}  # Set to store unique word types\n",
        "# for sentence in sentences:\n",
        "#     words = word_tokenize(sentence)\n",
        "#     tagged_words = pos_tag(words, tagset=\"universal\")\n",
        "\n",
        "#     # Extract the POS tags and add to the set\n",
        "#     words = [word for word, tag in tagged_words if tag not in [\"NOUN\", \"VERB\", \"ADJ\"]]\n",
        "#     for word in words:\n",
        "#       if word in word_counter:\n",
        "#         word_counter[word] += 1\n",
        "#       else:\n",
        "#         word_counter[word] = 1\n",
        "\n",
        "# df = pd.DataFrame(list(word_counter.items()), columns=[\"Word\", \"Count\"])\n",
        "# df = df.sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "# df.to_excel('all_OTHER_universal.xlsx', index=False)\n",
        "\n",
        "# half_len = len(df) // 2\n",
        "# df = df.head(half_len)\n",
        "# df.to_excel('allowed_OTHER.xlsx', index=False)"
      ],
      "metadata": {
        "id": "6_wUZHoypCBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB\n",
        "Algorithms:\n",
        "1. Load the original corpus\n",
        "2. Keep track of all the Noun words and how much they occur.\n",
        "3. Remove 50% of the least occuring Nouns, turn the keys to list.\n",
        "4. Perform prompting like usual, BUT, if it is a Noun, check if the word is in the ALLOWED_NOUN. If not, don't give the translation.\n",
        "\n",
        "Uses:\n",
        "1. allowed_adj.xlsx\n",
        "2. allowed_noun.xlsx\n",
        "3. allowed_verb.xlsx\n",
        "4. allowed_OTHER.xlsx"
      ],
      "metadata": {
        "id": "TjHoNyDiB1zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_allowed(path):\n",
        "  import pandas as pd\n",
        "\n",
        "  df = pd.read_excel(path)\n",
        "  word_list = df['Word'].astype(str).tolist()\n",
        "\n",
        "  return word_list"
      ],
      "metadata": {
        "id": "DUhcTtM66u3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def AB_bilingual_prompting(input_sentence, label_sentence, noising_map, compound_map, word_order, allowed_list, word_type):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_AB_bilingual_prompting\n",
        "  # Uses the tokenize function above\n",
        "  ### End of Explanation\n",
        "\n",
        "  assert word_type in [\"ADJ\", \"NOUN\", \"VERB\", \"OTHER\"], print(f\"{word_type} not in allowed word type\")\n",
        "\n",
        "  LABEL_TOKENS = tokenize(label_sentence)\n",
        "  R_NOISING_MAP = {}\n",
        "  for k,v in noising_map.items():\n",
        "    R_NOISING_MAP[v] = k\n",
        "\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  NOISED_TOKENS = []\n",
        "  COMPOUNDED_TOKENS = []\n",
        "\n",
        "  # word, tag = pos_tag([token], tagset=\"universal\")[0]\n",
        "  # if tag == word_type:\n",
        "  #   if word not in allowed_list:\n",
        "  #     continue\n",
        "\n",
        "  for token in LABEL_TOKENS:\n",
        "    if token in R_COMPOUND_MAP.keys():\n",
        "      COMPOUNDED_TOKENS.append(token)\n",
        "\n",
        "    if token in R_NOISING_MAP.keys():\n",
        "      NOISED_TOKENS.append(token)\n",
        "\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += \"The following is a list of word translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "\n",
        "  for token in COMPOUNDED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_COMPOUND_MAP[token][0]} {R_COMPOUND_MAP[token][1]}\" means \"{token}\"\\n'\n",
        "\n",
        "  for token in NOISED_TOKENS:\n",
        "    word, tag = pos_tag([R_NOISING_MAP[token]], tagset=\"universal\")[0]\n",
        "    if tag == word_type:\n",
        "      if word not in allowed_list:\n",
        "        continue\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_NOISING_MAP[token]}\" means \"{token}\"\\n'\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DBezjorDu47",
        "outputId": "ca76f04b-c4bd-45c9-f74a-2977c171cce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_AB_bilingual_prompting(input_file, label_files, noising_map, compound_map, allowed_list, word_type, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform bilingual prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'bilingual_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'bilingual_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file in label_files:\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = AB_bilingual_prompting(input_sentence, label_sentences[idx], noising_map, compound_map, EXPERIMENT_order, allowed_list, word_type)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "3T33eGb-9gui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ADJ"
      ],
      "metadata": {
        "id": "9GcwJJEI-exS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_english_svo')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'AB/AB_flores_dev_english_svo')\n",
        "]\n",
        "\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'AB/AB_noising_map.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'B/B_compound_map.pickle'))\n",
        "\n",
        "EN_DE_MAP = load_map(os.path.join(WORK_DIR, 'en_de_map.pickle'))\n",
        "EN_PT_MAP = load_map(os.path.join(WORK_DIR, 'en_pt_map.pickle'))\n",
        "EN_AF_MAP = load_map(os.path.join(WORK_DIR, 'en_af_map.pickle'))\n",
        "EN_GL_MAP = load_map(os.path.join(WORK_DIR, 'en_gl_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "YaefRmWa-v0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  allowed_list = load_allowed('allowed_adj.xlsx'),\n",
        "                                  word_type = \"ADJ\",\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"masked_adj\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0tVqkzl-5-R",
        "outputId": "19172485-5520-4316-ee8e-655ac327988a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/bilingual_prompting\n",
            "res_dir = experiment_results/AB/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:08<00:00, 124.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NOUN"
      ],
      "metadata": {
        "id": "JtScIXgR-hBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  allowed_list = load_allowed('allowed_noun.xlsx'),\n",
        "                                  word_type = \"NOUN\",\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"masked_noun\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHXE3oKXAUje",
        "outputId": "f72f0520-fb32-49e5-e8a9-fcf5d24f0d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/bilingual_prompting\n",
            "res_dir = experiment_results/AB/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:08<00:00, 116.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VERB"
      ],
      "metadata": {
        "id": "JgiqXXij-j0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  allowed_list = load_allowed('allowed_verb.xlsx'),\n",
        "                                  word_type = \"VERB\",\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"masked_verb\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4LOTRhvAYnG",
        "outputId": "7c2e9455-65d9-497b-b449-d43e441e3ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/bilingual_prompting\n",
            "res_dir = experiment_results/AB/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:07<00:00, 142.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OTHER"
      ],
      "metadata": {
        "id": "3oRj6VPx-l7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_AB_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                  label_files = LABEL_FILES,\n",
        "                                  noising_map = NOISING_MAP,\n",
        "                                  compound_map = COMPOUND_MAP,\n",
        "                                  allowed_list = load_allowed('allowed_OTHER.xlsx'),\n",
        "                                  word_type = \"OTHER\",\n",
        "                                  log_dir = LOG_DIR,\n",
        "                                  result_dir = RESULT_DIR,\n",
        "                                  log_fname = \"masked_other\",\n",
        "                                  result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ka_j3AAcI5",
        "outputId": "a030fd1c-0db5-4716-c371-a2c92520b1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = AB\n",
            "log_dir = experiment_prompts/AB/bilingual_prompting\n",
            "res_dir = experiment_results/AB/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:08<00:00, 120.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C/DxBA - D1"
      ],
      "metadata": {
        "id": "F_frA-hEA1Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def CDBA_bilingual_prompting(input_sentence, label_sentence, translate_map, compound_map, noising_map, word_order, allowed_list, word_type):\n",
        "  ### Start of Explanation\n",
        "  # Helper function for EXPERIMENT_CDBA_bilingual_prompting\n",
        "  # Uses the tokenize function above (tbh, they are all the same and unchanged)\n",
        "  ### End of Explanation\n",
        "\n",
        "  assert word_type in [\"ADJ\", \"NOUN\", \"VERB\", \"OTHER\"], print(f\"{word_type} not in allowed word type\")\n",
        "\n",
        "  # 0. Prepare prompt\n",
        "  prompt = \"Exurbanta is a lost language to humanity that was found only a few days ago.\\n\"\n",
        "  if word_order == \"sov\":\n",
        "    prompt += \"Exurbanta follows the Subject-Object-Verb word order.\\n\"\n",
        "  elif word_order == \"svo\":\n",
        "    prompt += \"Exurbanta follows the Subject-Verb-Object word order.\\n\"\n",
        "  elif word_order == \"vos\":\n",
        "    prompt += \"Exurbanta follows the Verb-Object-Subject word order .\\n\"\n",
        "  elif word_order == \"vso\":\n",
        "    prompt += \"Exurbanta follows the Verb-Subject-Object word order .\\n\"\n",
        "  prompt += \"The following is a list of word translations from English to Exurbanta:\\n\"\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "\n",
        "  INPUT_TOKENS = tokenize(input_sentence)\n",
        "  # 1. Original --> C/Dx\n",
        "  for token in INPUT_TOKENS:\n",
        "    word, tag = pos_tag([token], tagset=\"universal\")[0]\n",
        "    if token in translate_map:\n",
        "      if token not in ALREADY_TRANSLATED:\n",
        "        ALREADY_TRANSLATED.append(token)\n",
        "        ALREADY_TRANSLATED.append(TRANSLATE_MAP[token])\n",
        "\n",
        "        if tag == word_type:\n",
        "          if word not in allowed_list:\n",
        "            if TRANSLATE_MAP[token] in NOISING_MAP:\n",
        "              ALREADY_TRANSLATED.append(NOISING_MAP[TRANSLATE_MAP[token]])\n",
        "            continue\n",
        "\n",
        "        if TRANSLATE_MAP[token] in NOISING_MAP:\n",
        "          prompt += f'\"{token}\" means \"{NOISING_MAP[TRANSLATE_MAP[token]]}\"\\n'\n",
        "          ALREADY_TRANSLATED.append(NOISING_MAP[TRANSLATE_MAP[token]])\n",
        "        else:\n",
        "          prompt += f'\"{token}\" means \"{TRANSLATE_MAP[token]}\"\\n'\n",
        "\n",
        "\n",
        "  # 2. C/Dx --> C/DxB --> C/DxBA\n",
        "  ## Note, Even though the A + B experiment is named AB,\n",
        "  ## It actually perform compounding first, THEN noising\n",
        "  ## Just like this one.\n",
        "  LABEL_TOKENS = tokenize(label_sentence)\n",
        "\n",
        "  R_COMPOUND_MAP = {}\n",
        "  for k,v in compound_map.items():\n",
        "    R_COMPOUND_MAP[v] = k\n",
        "\n",
        "  R_NOISING_MAP = {}\n",
        "  for k,v in noising_map.items():\n",
        "    R_NOISING_MAP[v] = k\n",
        "\n",
        "  COMPOUNDED_TOKENS = []\n",
        "  NOISED_TOKENS = []\n",
        "\n",
        "  for token in LABEL_TOKENS:\n",
        "    if token in R_NOISING_MAP.keys():\n",
        "      NOISED_TOKENS.append(token)\n",
        "      if token in R_COMPOUND_MAP.keys():\n",
        "        COMPOUNDED_TOKENS.append(token)\n",
        "\n",
        "  for token in COMPOUNDED_TOKENS:\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_COMPOUND_MAP[token][0]} {R_COMPOUND_MAP[token][1]}\" means \"{token}\"\\n'\n",
        "\n",
        "  for token in NOISED_TOKENS:\n",
        "    word, tag = pos_tag([R_NOISING_MAP[token]], tagset=\"universal\")[0]\n",
        "    if tag == word_type:\n",
        "      if word not in allowed_list:\n",
        "        continue\n",
        "\n",
        "    if token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(token)\n",
        "      prompt += f'\"{R_NOISING_MAP[token]}\" means \"{token}\"\\n'\n",
        "\n",
        "  prompt += f'Translate the following text from English into Exurbanta:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQd1sB-jA7NB",
        "outputId": "59490cf5-a458-4f94-a27d-450b1a375471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_CDBA_bilingual_prompting(input_file, label_files, translate_map, compound_map, noising_map, allowed_list, word_type, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform bilingual prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'CDBA'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = label_files[0].split('/')[-1].split('_')[0]\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'bilingual_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'bilingual_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file in label_files:\n",
        "\n",
        "    ## Create folder preparations\n",
        "    EXPERIMENT_order = label_file.split('/')[-1].split('_')[-1]\n",
        "    EXPERIMENT_order_logdir = os.path.join(EXPERIMENT_logdir, EXPERIMENT_order)\n",
        "    EXPERIMENT_order_resdir = os.path.join(EXPERIMENT_resdir, EXPERIMENT_order)\n",
        "    os.makedirs(EXPERIMENT_order_logdir, exist_ok=True)\n",
        "    os.makedirs(EXPERIMENT_order_resdir, exist_ok=True)\n",
        "    ## End of Experiment Preparation\n",
        "\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = CDBA_bilingual_prompting(input_sentence, label_sentences[idx], translate_map, compound_map, noising_map, EXPERIMENT_order, allowed_list, word_type)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "zoo8gyUPHs3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ADJ"
      ],
      "metadata": {
        "id": "stWaCtdRH-w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WORK_DIR = \"/content/extract\"\n",
        "\n",
        "INPUT_FILE = os.path.join(WORK_DIR, 'flores_english_svo')\n",
        "FEWSHOT_INPUT_FILE = os.path.join(WORK_DIR, 'flores_dev_english_svo')\n",
        "\n",
        "LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_english_svo')\n",
        "]\n",
        "\n",
        "FEWSHOT_LABEL_FILES = [\n",
        "    os.path.join(WORK_DIR, 'D1BA/D1BA_flores_dev_english_svo')\n",
        "]\n",
        "\n",
        "TRANSLATE_MAP = load_map(os.path.join(WORK_DIR, 'D1_mapping.pickle'))\n",
        "COMPOUND_MAP = load_map(os.path.join(WORK_DIR, 'D1B/D1B_compound_map.pickle'))\n",
        "NOISING_MAP = load_map(os.path.join(WORK_DIR, 'D1BA/D1BA_noising_map.pickle'))\n",
        "\n",
        "LOG_DIR = \"experiment_prompts/\"\n",
        "RESULT_DIR = \"experiment_results/\""
      ],
      "metadata": {
        "id": "R1CiC8HGIF4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                    label_files = LABEL_FILES,\n",
        "                                    translate_map = TRANSLATE_MAP,\n",
        "                                    compound_map = COMPOUND_MAP,\n",
        "                                    noising_map = NOISING_MAP,\n",
        "                                    allowed_list = load_allowed('allowed_adj.xlsx'),\n",
        "                                    word_type = \"ADJ\",\n",
        "                                    log_dir = LOG_DIR,\n",
        "                                    result_dir = RESULT_DIR,\n",
        "                                    log_fname = \"masked_adj\",\n",
        "                                    result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unubB-FsIJlk",
        "outputId": "dd70a7f7-d6ac-4d82-cdbd-aaba6b85fe0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/bilingual_prompting\n",
            "res_dir = experiment_results/D1BA/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:14<00:00, 68.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NOUN"
      ],
      "metadata": {
        "id": "aGwuInreH_zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                    label_files = LABEL_FILES,\n",
        "                                    translate_map = TRANSLATE_MAP,\n",
        "                                    compound_map = COMPOUND_MAP,\n",
        "                                    noising_map = NOISING_MAP,\n",
        "                                    allowed_list = load_allowed('allowed_noun.xlsx'),\n",
        "                                    word_type = \"NOUN\",\n",
        "                                    log_dir = LOG_DIR,\n",
        "                                    result_dir = RESULT_DIR,\n",
        "                                    log_fname = \"masked_noun\",\n",
        "                                    result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqVNbsjld_38",
        "outputId": "14f6d32c-c7ed-4f5d-e8c5-b1d957c5a498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/bilingual_prompting\n",
            "res_dir = experiment_results/D1BA/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:14<00:00, 70.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VERB"
      ],
      "metadata": {
        "id": "t8ffR9LZIBC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                    label_files = LABEL_FILES,\n",
        "                                    translate_map = TRANSLATE_MAP,\n",
        "                                    compound_map = COMPOUND_MAP,\n",
        "                                    noising_map = NOISING_MAP,\n",
        "                                    allowed_list = load_allowed('allowed_verb.xlsx'),\n",
        "                                    word_type = \"VERB\",\n",
        "                                    log_dir = LOG_DIR,\n",
        "                                    result_dir = RESULT_DIR,\n",
        "                                    log_fname = \"masked_verb\",\n",
        "                                    result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0foOe0EjQQ_",
        "outputId": "88d8f22b-854f-4fb7-963c-835b317255f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/bilingual_prompting\n",
            "res_dir = experiment_results/D1BA/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:14<00:00, 70.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OTHER"
      ],
      "metadata": {
        "id": "-nq5qAd5ICN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_CDBA_bilingual_prompting(input_file = INPUT_FILE,\n",
        "                                    label_files = LABEL_FILES,\n",
        "                                    translate_map = TRANSLATE_MAP,\n",
        "                                    compound_map = COMPOUND_MAP,\n",
        "                                    noising_map = NOISING_MAP,\n",
        "                                    allowed_list = load_allowed('allowed_OTHER.xlsx'),\n",
        "                                    word_type = \"OTHER\",\n",
        "                                    log_dir = LOG_DIR,\n",
        "                                    result_dir = RESULT_DIR,\n",
        "                                    log_fname = \"masked_other\",\n",
        "                                    result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvItSsUzjUgy",
        "outputId": "a42f958c-636b-4aff-fb5d-02007804b589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = D1BA\n",
            "log_dir = experiment_prompts/D1BA/bilingual_prompting\n",
            "res_dir = experiment_results/D1BA/bilingual_prompting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:12<00:00, 78.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts v3\n",
        "English to X (Code: EX)"
      ],
      "metadata": {
        "id": "gkZ8GbVmHvv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EX"
      ],
      "metadata": {
        "id": "GVyW5xDjIBC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fewshot"
      ],
      "metadata": {
        "id": "qLikwfKEyvD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper func\n",
        "def load_map(f_name):\n",
        "  import pickle\n",
        "  with open(f_name, 'rb') as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "def load_file(f_name):\n",
        "  print(f\"attempting to open {f_name}\")\n",
        "  with open(f_name, 'r') as f:\n",
        "    return f.readlines()\n",
        "## End of helper func"
      ],
      "metadata": {
        "id": "-04FO30PJtJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def get_index(in_token, few_shots):\n",
        "  for idx, tokens in few_shots:\n",
        "    if in_token in tokens:\n",
        "      return idx\n",
        "  return in_token\n",
        "\n",
        "import random\n",
        "random.seed(2023)\n",
        "def EX_fewshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, translation_map, language):\n",
        "  fewshot_input_sentences = []\n",
        "  with open(fewshot_input_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_input_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  fewshot_label_sentences = []\n",
        "  with open(fewshot_label_file, 'r') as f:\n",
        "    i = 1\n",
        "    for line in f:\n",
        "      fewshot_label_sentences.append((i, line.strip()))\n",
        "      i += 1\n",
        "\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "  few_shot_indexes = []\n",
        "  unfound_word = []\n",
        "  for input_token in input_tokens:\n",
        "    idx = get_index(input_token, fewshot_input_sentences)\n",
        "    if isinstance(idx, int):\n",
        "      few_shot_indexes.append(get_index(input_token, fewshot_input_sentences))\n",
        "    elif isinstance(idx, str):\n",
        "      unfound_word.append(idx)\n",
        "\n",
        "  few_shot_indexes = list(set(few_shot_indexes))\n",
        "  unfound_word = list(set(unfound_word)) # Not handled\n",
        "\n",
        "  prompt = f\"The following is a list of sentence translations from English to {language}:\\n\"\n",
        "  for idx in few_shot_indexes:\n",
        "    prompt += f\"English: {fewshot_input_sentences[idx-1][1]}\\n\"\n",
        "    prompt += f\"{language}: {fewshot_label_sentences[idx-1][1]}\\n\"\n",
        "\n",
        "  for unfound_en_word in unfound_word:\n",
        "    prompt += f\"English: {unfound_en_word}\\n\"\n",
        "    prompt += f\"{language}: {translation_map.get(unfound_en_word, unfound_en_word)}\\n\"\n",
        "\n",
        "  prompt += f'Translate the following text from English into {language}:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "OrkHnXAnH2Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_EX_fewshot_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translation_map, language, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = \"ES\"\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = EX_fewshot_prompting(input_sentence, fewshot_input_file, fewshot_label_file, translation_map, language)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "      EXPERIMENT_order_logdir = os.getcwd()\n",
        "      EXPERIMENT_order_resdir = os.getcwd()\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "OszOmcmGIoOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_fewshot_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/eus_Latn.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/eus_Latn.dev\"],\n",
        "                                translation_map = load_map(\"/content/EB_mapping.pickle\"),\n",
        "                                language = \"Basque\"\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"EB_prompt_fewshot\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKhr_ZywJPHC",
        "outputId": "ec3970f3-ca6d-455f-b71b-b54425a65453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/eus_Latn.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:04<00:00, 203.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bilingual"
      ],
      "metadata": {
        "id": "29fYVsmmysS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  import re\n",
        "  import string\n",
        "  ret = []\n",
        "  for token in text.split(' '):\n",
        "    result_list = re.findall(r'\\w+|[^\\w\\s]', token)\n",
        "    flag = 0\n",
        "    for token in result_list:\n",
        "      if token in ['(', '[', '{', '}', ']', ')', '\"', \"'\"]:\n",
        "        flag = 1\n",
        "    if flag:\n",
        "      ret.append(''.join(result_list))\n",
        "    elif len(result_list) > 2:\n",
        "      ret.append(''.join(result_list))\n",
        "    else:\n",
        "      ret.extend(result_list)\n",
        "  return ret\n",
        "\n",
        "def EX_bilingual_prompting(input_sentence, translation_map, language):\n",
        "  prompt = f\"The following is a list of word translations from English to {language}:\\n\"\n",
        "  input_tokens = tokenize(input_sentence)\n",
        "\n",
        "  ALREADY_TRANSLATED = []\n",
        "  for input_token in input_tokens:\n",
        "    if input_token not in ALREADY_TRANSLATED:\n",
        "      ALREADY_TRANSLATED.append(input_token)\n",
        "      prompt += f'\"{input_token}\" means \"{translation_map.get(input_token, input_token)}\"\\n'\n",
        "\n",
        "  prompt += f'Translate the following text from English into {language}:\\n{input_sentence}'\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "NuM_BUGKVvmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EXPERIMENT_EX_bilingual_prompting(input_file, label_files, fewshot_input_file, fewshot_label_files, translation_map, language, log_dir, result_dir, log_fname = \"\", result_fname = \"\"):\n",
        "  ### Start of Explanation\n",
        "  # This code is used to perform fewshot prompting experiment\n",
        "  # ONLY FOR EXPERIMENT WITH CODE 'AB'\n",
        "  ### End of Explanation\n",
        "\n",
        "  # Start of Experiment Preparation\n",
        "  import os\n",
        "  from tqdm import tqdm\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "  ## Get Experiment Details\n",
        "  EXPERIMENT_name  = \"ES\"\n",
        "  EXPERIMENT_logdir = os.path.join(log_dir, EXPERIMENT_name)\n",
        "  EXPERIMENT_resdir = os.path.join(result_dir, EXPERIMENT_name)\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  EXPERIMENT_logdir = os.path.join(EXPERIMENT_logdir, 'fewshot_prompting')\n",
        "  EXPERIMENT_resdir = os.path.join(EXPERIMENT_resdir, 'fewshot_prompting')\n",
        "\n",
        "  os.makedirs(EXPERIMENT_logdir, exist_ok=True)\n",
        "  os.makedirs(EXPERIMENT_resdir, exist_ok=True)\n",
        "\n",
        "  ### Start of Debug\n",
        "  print(f\"name = {EXPERIMENT_name}\")\n",
        "  print(f\"log_dir = {EXPERIMENT_logdir}\")\n",
        "  print(f\"res_dir = {EXPERIMENT_resdir}\")\n",
        "  ### End of Debug\n",
        "\n",
        "  input_sentences = load_file(input_file)\n",
        "  for label_file, fewshot_label_file in zip(label_files, fewshot_label_files):\n",
        "    # Start Experiment\n",
        "    label_sentences = load_file(label_file)\n",
        "    assert len(input_sentences) == len(label_sentences), print(\"FILE LENGTH DONT MATCH\")\n",
        "\n",
        "    prompts = []\n",
        "    for idx, input_sentence in enumerate(tqdm(input_sentences)):\n",
        "      prompt = EX_bilingual_prompting(input_sentence, translation_map, language)\n",
        "      dialog = [\n",
        "          {'role': 'system', 'content': 'You can only use one sentence.'},\n",
        "          {'role': 'user', 'content': prompt}\n",
        "      ]\n",
        "      payload = {\n",
        "        \"inputs\": [dialog],\n",
        "        \"parameters\": {\"max_new_tokens\": 384, \"top_p\": 0.9, \"temperature\": 0.01}\n",
        "      }\n",
        "      # result = query_endpoint(payload)[0]['generation']['content']\n",
        "      result = \"test\"\n",
        "      # temp_log = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_log = f\"{prompt}\\n\"\n",
        "      # temp_log += f\"{result}\\n\"\n",
        "      temp_log += f\"=====\\n\"\n",
        "\n",
        "      temp_out = f\"$$$ Entry {idx}\\n\"\n",
        "      temp_out += f\"{result}\\n\"\n",
        "      temp_out += f\"--ENDOFENTRY--\\n\"\n",
        "\n",
        "      EXPERIMENT_order_logdir = os.getcwd()\n",
        "      EXPERIMENT_order_resdir = os.getcwd()\n",
        "      if log_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, f'log'), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_logdir, log_fname), 'a') as f:\n",
        "          f.write(temp_log)\n",
        "\n",
        "      if result_fname == \"\":\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, f'result'), 'a') as f:\n",
        "          f.write(temp_out)\n",
        "      else:\n",
        "        with open(os.path.join(EXPERIMENT_order_resdir, result_fname), 'a') as f:\n",
        "          f.write(temp_out)"
      ],
      "metadata": {
        "id": "uz21E0mRWpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_bilingual_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/eus_Latn.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/eus_Latn.dev\"],\n",
        "                                translation_map = load_map(\"/content/EB_mapping.pickle\"),\n",
        "                                language = \"Basque\"\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"EB_prompt_bilingual\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "id": "q9i-kTi8XF3V",
        "outputId": "f5ec7142-25b7-4bcd-b60b-6d5b29c54a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/eus_Latn.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:00<00:00, 4286.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English --> Afrikaans\n"
      ],
      "metadata": {
        "id": "vWYin0ujzc-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_fewshot_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/afr_Latn.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/afr_Latn.dev\"],\n",
        "                                translation_map = load_map(\"/content/en_af_map.pickle\"),\n",
        "                                language = \"Afrikaans\",\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"en_af_prompt_fewshot\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oacE-vA6M3e",
        "outputId": "1d4ac598-7156-438e-d648-f95371108a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/afr_Latn.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:03<00:00, 283.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_bilingual_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/afr_Latn.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/afr_Latn.dev\"],\n",
        "                                translation_map = load_map(\"/content/en_af_map.pickle\"),\n",
        "                                language = \"Afrikaans\",\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"en_af_prompt_bilingual\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVKI3B9o73NE",
        "outputId": "9d1cf78d-3bd2-45c3-f8f6-e5b6c34d823e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/afr_Latn.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:00<00:00, 4960.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English --> Tamil"
      ],
      "metadata": {
        "id": "rLHeaWR-zfHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_fewshot_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/tam_Taml.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/tam_Taml.dev\"],\n",
        "                                translation_map = load_map(\"/content/en_ta_mapping.pickle\"),\n",
        "                                language = \"Tamil\",\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"en_ta_prompt_fewshot\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auga4DFi6NhQ",
        "outputId": "51d33fd0-62f7-41a4-dafb-9200f0979531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/tam_Taml.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:05<00:00, 179.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_bilingual_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/tam_Taml.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/tam_Taml.dev\"],\n",
        "                                translation_map = load_map(\"/content/en_ta_mapping.pickle\"),\n",
        "                                language = \"Tamil\",\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"en_ta_prompt_bilingual\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzdIWKl88MeZ",
        "outputId": "af224c01-aebf-4076-be13-0c2cc72cced9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/tam_Taml.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:00<00:00, 4620.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English --> Telugu"
      ],
      "metadata": {
        "id": "3aYQwc7Yzh9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_fewshot_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/tel_Telu.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/tel_Telu.dev\"],\n",
        "                                translation_map = load_map(\"/content/en_te_mapping.pickle\"),\n",
        "                                language = \"Telugu\",\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"en_te_prompt_fewshot\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXtvz-Tw6OMn",
        "outputId": "d2a70a33-e2a6-4224-9e5d-1a34fb0af011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/tel_Telu.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:04<00:00, 232.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_EX_bilingual_prompting(input_file = \"/content/eng_Latn.devtest\",\n",
        "                                label_files = [\"/content/tel_Telu.devtest\"],\n",
        "                                fewshot_input_file = \"/content/eng_Latn.dev\",\n",
        "                                fewshot_label_files = [\"/content/tel_Telu.dev\"],\n",
        "                                translation_map = load_map(\"/content/en_te_mapping.pickle\"),\n",
        "                                language = \"Telugu\",\n",
        "                                log_dir = \"Prompts\",\n",
        "                                result_dir = \" Results\",\n",
        "                                log_fname = \"en_te_prompt_bilingual\",\n",
        "                                result_fname = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWGHTXAL8Qp0",
        "outputId": "d06faff1-4171-4955-d5b1-c9dbed9c9f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name = ES\n",
            "log_dir = Prompts/ES/fewshot_prompting\n",
            "res_dir =  Results/ES/fewshot_prompting\n",
            "attempting to open /content/eng_Latn.devtest\n",
            "attempting to open /content/tel_Telu.devtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1012/1012 [00:00<00:00, 2434.23it/s]\n"
          ]
        }
      ]
    }
  ]
}